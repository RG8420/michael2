{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfe2607-26bd-4a18-acbd-2fceb89e9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyswarm in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyswarm) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd830da6-08ea-490b-90b4-208ea7af9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyswarm import pso\n",
    "\n",
    "RANDOM_STATE = 8420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae350ba-88d6-483c-abe5-3588d9d7a965",
   "metadata": {},
   "source": [
    "## **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732cd453-9697-4828-b3cf-0cea50b01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vietnam_path = \"dataset/data_vietnam.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532e826b-eb76-4f35-ba8e-057b5b65ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load arrays from the .npz file\n",
    "vietnam_data = np.load(vietnam_path)\n",
    "\n",
    "# Access individual arrays\n",
    "x_vietnam = vietnam_data['x']\n",
    "coordinate_vietnam = vietnam_data[\"coordinates\"]\n",
    "y_classification_vietnam = vietnam_data['y_classification']\n",
    "y_regression_vietnam = vietnam_data['y_regression']\n",
    "columns_vietnam = vietnam_data['columns']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12de6df9-1cc9-4e7a-896f-a64632591f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_vietnam\n",
    "coordinates = coordinate_vietnam\n",
    "y = y_classification_vietnam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef8c65b-8384-4baf-85f0-f148619bc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, coordinates_train, coordinates_test, y_train, y_test = train_test_split(x, coordinates, y, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec1b957-1596-4b65-8b5a-c34d0c851f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_column = [\"na\", \"k\", \"ca2\", \"mg2\", \"fe3\", \"fe2\", \"al3\", \"cl\", \"so4\", \"hco3\", \"co3\", \"no2\", \"no3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee872663-5f26-41d0-b7ee-411dce673256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x_vietnam, columns=columns_vietnam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbd0aae-1ea8-430c-a3a7-9a7817e55d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['well_code', 'date_sampling', 'quarter', 'type_analyzing',\n",
       "       'date_analyzing', 'laboratory', 'number_analyzing', 'na', 'k', 'ca2',\n",
       "       'mg2', 'fe3', 'fe2', 'al3', 'cl', 'so4', 'hco3', 'co3', 'no2',\n",
       "       'hardness_general', 'no3', 'hardness_temporal', 'hardness_permanent',\n",
       "       'ph', 'co2_free', 'co2_depend', 'co2_infiltrate', 'sio2', 'color',\n",
       "       'smell', 'tatse', 'tds105'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aafaac6-fb3d-49fc-96ce-a0c9e14f9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2346d41-14cb-45b8-b3d0-44c62a115c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91eb4e93-7fe0-4b6c-b055-6a4447cab74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b41633-d98b-46a6-ad9a-58a04c01dd1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Model Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a750bf-7294-4177-aedc-3fb11d00398b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Implementation of the Haversine Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feeaa9dd-26bd-4733-ba2d-cb40bcdaf058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Haversine layer\n",
    "class HaversineLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        lat, lon = inputs[..., 0], inputs[..., 1]\n",
    "        return tf.expand_dims(tf.sqrt(lat ** 2 + lon ** 2), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6b44c0-8a94-4938-8e81-0e2310a03465",
   "metadata": {},
   "outputs": [],
   "source": [
    "hav_inp = np.array([[1.35, 1.7],\n",
    "                    [2.72, 1.6],\n",
    "                    [3.4, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fb345b-f30c-4d43-9f18-a487cc8a2c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1708293]\n",
      " [3.1556933]\n",
      " [3.6055512]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to a TensorFlow tensor\n",
    "input_tensor = tf.convert_to_tensor(hav_inp, dtype=tf.float32)\n",
    "\n",
    "# Instantiate the Haversine layer\n",
    "haversine_layer = HaversineLayer()\n",
    "\n",
    "# Pass the tensor through the Haversine layer\n",
    "output_tensor = haversine_layer(input_tensor)\n",
    "\n",
    "# Print the output tensor\n",
    "print(output_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01addc89-b303-4c02-87c9-f60aecbf7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48a8d7-bd8b-4c6f-9dd9-6bc236074ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b37540-f86f-4983-b54a-43285ebf366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the multiclass labels\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e1285-3dbf-4149-b753-bd5f290010fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Implementation of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8eeaa158-589b-4fbe-b6f0-e7b6c5f3fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, layer_params):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_classes = y_train_one_hot.shape[1] # num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=self.num_embedding_dims + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(layer_params[0], 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(layer_params[1], activation='relu') # 128\n",
    "        self.dense_layer2 = layers.Dense(layer_params[2], activation='relu') # 64\n",
    "        self.output_layer = layers.Dense(self.num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        # print(\"Input features Shape: \", input_features.shape)\n",
    "        # print(\"Input coordinates Shape: \", input_coordinates.shape)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = self.haversine_layer(input_coordinates)\n",
    "        # print(\"Haversine layer output shape: \", haversine_output.shape)\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([embedding_output, haversine_output])\n",
    "        # print(\"Combined Features Shape: \", combined_features.shape)\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        # print(\"Reshaped Features Shape: \", reshaped_features_for_attention.shape)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        # print(\"Attention Layer Shape: \", multi_head_attention.shape)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(multi_head_attention)\n",
    "        # print(\"Flattened Layer shape: \", flattened_attention.shape)\n",
    "        \n",
    "        concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        # print(\"Concat Features Shape: \", concat_features.shape)\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        concat_features = tf.expand_dims(concat_features, axis=-1)\n",
    "        # print(\"Expand Dims Features shape: \", concat_features.shape)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        space_conv = self.space_conv(concat_features)\n",
    "        # print(\"Convolution Layer Shape: \", space_conv.shape)\n",
    "        \n",
    "        # Global max pooling\n",
    "        pooled = self.global_pooling_layer(space_conv)\n",
    "        # print(\"Pooling Layer Shape: \", pooled.shape)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(pooled)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        # print(\"Output Shape: \", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea61101-0a56-46af-8906-5ab843b92bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Training of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "610a76b1-0af6-408b-9233-d242c4f17dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_params):\n",
    "    model = MyModel(layer_params)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f59b5b6d-1124-4831-bc25-a19dd5f7bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(layer_params):\n",
    "    model = create_model(layer_params)\n",
    "    hst = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)\n",
    "    return 1 - hst.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2365e-e80e-4342-8a0c-3018ef9f018a",
   "metadata": {},
   "source": [
    "#### **Incorporation of PSO Optimization (Function imported from PySwarm Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "270067a8-0289-4478-a852-6e14b46c7da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9333 - accuracy: 0.2715 - val_loss: 1.5549 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5181 - accuracy: 0.3463 - val_loss: 1.5002 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0884 - accuracy: 0.3140 - val_loss: 1.6453 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5328 - accuracy: 0.3302 - val_loss: 1.5038 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.9134 - accuracy: 0.2700 - val_loss: 1.5155 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5213 - accuracy: 0.3316 - val_loss: 1.5105 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.3729 - accuracy: 0.3302 - val_loss: 1.7228 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6228 - accuracy: 0.2942 - val_loss: 1.5145 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.7324 - accuracy: 0.3045 - val_loss: 1.5131 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5291 - accuracy: 0.3324 - val_loss: 1.5082 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7706 - accuracy: 0.3228 - val_loss: 1.5060 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5176 - accuracy: 0.3390 - val_loss: 1.5068 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8838 - accuracy: 0.2935 - val_loss: 1.5159 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5227 - accuracy: 0.3324 - val_loss: 1.5043 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 34ms/step - loss: 2.0385 - accuracy: 0.2722 - val_loss: 1.6713 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5358 - accuracy: 0.3324 - val_loss: 1.5026 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 4.4997 - accuracy: 0.1020 - val_loss: 3.3546 - val_accuracy: 0.0954\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 2.4885 - accuracy: 0.1108 - val_loss: 2.0635 - val_accuracy: 0.2075\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.4851 - accuracy: 0.2531 - val_loss: 1.5404 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5317 - accuracy: 0.3338 - val_loss: 1.5236 - val_accuracy: 0.2448\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6300 - accuracy: 0.3169 - val_loss: 1.5017 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5172 - accuracy: 0.3346 - val_loss: 1.5021 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.7262 - accuracy: 0.2979 - val_loss: 2.2615 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8538 - accuracy: 0.3243 - val_loss: 1.5902 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.5566 - accuracy: 0.3287 - val_loss: 1.7011 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5385 - accuracy: 0.3316 - val_loss: 1.5114 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.7629 - accuracy: 0.1761 - val_loss: 1.5494 - val_accuracy: 0.3278\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5372 - accuracy: 0.3302 - val_loss: 1.5073 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.2966 - accuracy: 0.2531 - val_loss: 1.5244 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5266 - accuracy: 0.3309 - val_loss: 1.5062 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6632 - accuracy: 0.2883 - val_loss: 1.5127 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5199 - accuracy: 0.3324 - val_loss: 1.5050 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0217 - accuracy: 0.2421 - val_loss: 1.5326 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5230 - accuracy: 0.3324 - val_loss: 1.5060 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.1080 - accuracy: 0.1937 - val_loss: 1.5016 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5166 - accuracy: 0.3316 - val_loss: 1.4979 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.7306 - accuracy: 0.2583 - val_loss: 1.5071 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5128 - accuracy: 0.3309 - val_loss: 1.5014 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6096 - accuracy: 0.2986 - val_loss: 1.5048 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5156 - accuracy: 0.3309 - val_loss: 1.5155 - val_accuracy: 0.2199\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6775 - accuracy: 0.3280 - val_loss: 1.5220 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5262 - accuracy: 0.3316 - val_loss: 1.5119 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5307 - accuracy: 0.3272 - val_loss: 1.5062 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5180 - accuracy: 0.3324 - val_loss: 1.5143 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5434 - accuracy: 0.3228 - val_loss: 1.5065 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5178 - accuracy: 0.3287 - val_loss: 1.5123 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.4970 - accuracy: 0.1959 - val_loss: 1.6026 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5313 - accuracy: 0.3133 - val_loss: 1.5111 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8549 - accuracy: 0.3045 - val_loss: 1.5350 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5243 - accuracy: 0.3309 - val_loss: 1.5062 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0465 - accuracy: 0.2883 - val_loss: 1.5035 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5200 - accuracy: 0.3324 - val_loss: 1.5043 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5652 - accuracy: 0.3015 - val_loss: 1.5112 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5231 - accuracy: 0.3331 - val_loss: 1.5142 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6586 - accuracy: 0.2898 - val_loss: 1.5098 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5176 - accuracy: 0.3294 - val_loss: 1.5023 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6537 - accuracy: 0.3118 - val_loss: 1.5042 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5180 - accuracy: 0.3294 - val_loss: 1.5026 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.6625 - accuracy: 0.2627 - val_loss: 1.5149 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5192 - accuracy: 0.3324 - val_loss: 1.5050 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5845 - accuracy: 0.2920 - val_loss: 1.5150 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5231 - accuracy: 0.3353 - val_loss: 1.5109 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.2566 - accuracy: 0.2847 - val_loss: 1.6602 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5477 - accuracy: 0.3177 - val_loss: 1.5089 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9103 - accuracy: 0.3103 - val_loss: 1.5099 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5165 - accuracy: 0.3368 - val_loss: 1.5008 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0713 - accuracy: 0.2876 - val_loss: 1.5436 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5242 - accuracy: 0.3096 - val_loss: 1.5022 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 3.4222 - accuracy: 0.2054 - val_loss: 1.7599 - val_accuracy: 0.3237\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5679 - accuracy: 0.3155 - val_loss: 1.5106 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.1852 - accuracy: 0.2326 - val_loss: 1.5467 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5324 - accuracy: 0.3250 - val_loss: 1.5111 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.2841 - accuracy: 0.2847 - val_loss: 1.5848 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5333 - accuracy: 0.3258 - val_loss: 1.5163 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.2813 - accuracy: 0.3324 - val_loss: 1.7085 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5833 - accuracy: 0.3316 - val_loss: 1.5096 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.6435 - accuracy: 0.3030 - val_loss: 1.7866 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5759 - accuracy: 0.3302 - val_loss: 1.5125 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.1345 - accuracy: 0.3206 - val_loss: 1.6276 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5344 - accuracy: 0.3309 - val_loss: 1.5124 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6403 - accuracy: 0.2869 - val_loss: 1.5131 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5279 - accuracy: 0.3250 - val_loss: 1.5068 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.9543 - accuracy: 0.2421 - val_loss: 1.7862 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.7375 - accuracy: 0.3324 - val_loss: 1.5866 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8581 - accuracy: 0.2905 - val_loss: 1.5143 - val_accuracy: 0.3361\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5214 - accuracy: 0.3375 - val_loss: 1.5068 - val_accuracy: 0.3568\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9247 - accuracy: 0.2553 - val_loss: 1.5216 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5190 - accuracy: 0.3272 - val_loss: 1.5106 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.0220 - accuracy: 0.3169 - val_loss: 1.6178 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5621 - accuracy: 0.3280 - val_loss: 1.5044 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8577 - accuracy: 0.2854 - val_loss: 1.5218 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5195 - accuracy: 0.3316 - val_loss: 1.5061 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.2177 - accuracy: 0.2487 - val_loss: 1.5396 - val_accuracy: 0.3320\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5359 - accuracy: 0.3221 - val_loss: 1.5155 - val_accuracy: 0.3154\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9370 - accuracy: 0.2539 - val_loss: 1.5254 - val_accuracy: 0.3029\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5281 - accuracy: 0.3081 - val_loss: 1.5035 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5232 - accuracy: 0.3243 - val_loss: 1.5097 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5183 - accuracy: 0.3243 - val_loss: 1.5226 - val_accuracy: 0.3568\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5807 - accuracy: 0.2847 - val_loss: 1.5180 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5279 - accuracy: 0.3316 - val_loss: 1.5104 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5634 - accuracy: 0.3147 - val_loss: 1.5122 - val_accuracy: 0.3112\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5209 - accuracy: 0.3236 - val_loss: 1.5121 - val_accuracy: 0.3071\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 33ms/step - loss: 2.3171 - accuracy: 0.3140 - val_loss: 1.8642 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6399 - accuracy: 0.3302 - val_loss: 1.5068 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6313 - accuracy: 0.3272 - val_loss: 1.5119 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5168 - accuracy: 0.3302 - val_loss: 1.4987 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.6130 - accuracy: 0.2355 - val_loss: 2.1862 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.7494 - accuracy: 0.3096 - val_loss: 1.5109 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6129 - accuracy: 0.3236 - val_loss: 1.5272 - val_accuracy: 0.1992\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5307 - accuracy: 0.3074 - val_loss: 1.5180 - val_accuracy: 0.2822\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 34ms/step - loss: 1.9146 - accuracy: 0.2913 - val_loss: 1.5461 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5322 - accuracy: 0.3324 - val_loss: 1.5071 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8219 - accuracy: 0.2913 - val_loss: 1.5039 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5203 - accuracy: 0.3324 - val_loss: 1.5022 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.8881 - accuracy: 0.2487 - val_loss: 1.7018 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5481 - accuracy: 0.3103 - val_loss: 1.5155 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.1041 - accuracy: 0.2480 - val_loss: 1.5181 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5263 - accuracy: 0.3316 - val_loss: 1.5074 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9481 - accuracy: 0.3338 - val_loss: 1.5159 - val_accuracy: 0.3361\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5164 - accuracy: 0.3368 - val_loss: 1.4995 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9611 - accuracy: 0.2876 - val_loss: 1.5509 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5352 - accuracy: 0.3309 - val_loss: 1.5028 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.7399 - accuracy: 0.2634 - val_loss: 1.5164 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5168 - accuracy: 0.3324 - val_loss: 1.5043 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.1700 - accuracy: 0.2832 - val_loss: 1.6073 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5429 - accuracy: 0.3324 - val_loss: 1.5039 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8625 - accuracy: 0.3199 - val_loss: 1.5134 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5372 - accuracy: 0.3089 - val_loss: 1.5117 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9138 - accuracy: 0.2700 - val_loss: 1.5536 - val_accuracy: 0.2282\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5334 - accuracy: 0.3089 - val_loss: 1.5062 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8287 - accuracy: 0.2678 - val_loss: 1.5143 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5243 - accuracy: 0.3324 - val_loss: 1.5075 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.9194 - accuracy: 0.2370 - val_loss: 1.5339 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5322 - accuracy: 0.3324 - val_loss: 1.5027 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.2943 - accuracy: 0.2010 - val_loss: 1.5020 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5158 - accuracy: 0.3338 - val_loss: 1.4976 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.0251 - accuracy: 0.1893 - val_loss: 1.6680 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5402 - accuracy: 0.3133 - val_loss: 1.5167 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 3.2494 - accuracy: 0.2113 - val_loss: 2.0365 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.8044 - accuracy: 0.3177 - val_loss: 1.5831 - val_accuracy: 0.3320\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.0662 - accuracy: 0.2311 - val_loss: 1.5242 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5267 - accuracy: 0.3236 - val_loss: 1.5304 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8907 - accuracy: 0.2472 - val_loss: 1.5122 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5199 - accuracy: 0.3331 - val_loss: 1.5060 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6238 - accuracy: 0.3177 - val_loss: 1.5105 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5181 - accuracy: 0.3434 - val_loss: 1.5090 - val_accuracy: 0.3568\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.2921 - accuracy: 0.3177 - val_loss: 1.7597 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5843 - accuracy: 0.3213 - val_loss: 1.5043 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.8044 - accuracy: 0.3324 - val_loss: 1.5080 - val_accuracy: 0.3278\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.5203 - accuracy: 0.3199 - val_loss: 1.5085 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.1406 - accuracy: 0.2971 - val_loss: 1.5667 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5317 - accuracy: 0.3162 - val_loss: 1.5063 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.8164 - accuracy: 0.2370 - val_loss: 2.3514 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8878 - accuracy: 0.3324 - val_loss: 1.5238 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.1510 - accuracy: 0.3081 - val_loss: 1.7042 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5673 - accuracy: 0.3155 - val_loss: 1.5061 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 2.0464 - accuracy: 0.2392 - val_loss: 1.6021 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5361 - accuracy: 0.3331 - val_loss: 1.5080 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6471 - accuracy: 0.3191 - val_loss: 1.5067 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.5187 - accuracy: 0.3302 - val_loss: 1.5035 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 1.6877 - accuracy: 0.2795 - val_loss: 1.5107 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 1.5216 - accuracy: 0.3324 - val_loss: 1.5048 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 2.7816 - accuracy: 0.2120 - val_loss: 1.5187 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.5312 - accuracy: 0.3346 - val_loss: 1.5085 - val_accuracy: 0.3320\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.6191 - accuracy: 0.1937 - val_loss: 1.5186 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5377 - accuracy: 0.3331 - val_loss: 1.5072 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.0254 - accuracy: 0.1922 - val_loss: 1.5627 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5357 - accuracy: 0.3265 - val_loss: 1.5070 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 6.1516 - accuracy: 0.2377 - val_loss: 2.9979 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.1518 - accuracy: 0.3331 - val_loss: 1.5308 - val_accuracy: 0.3154\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6238 - accuracy: 0.2957 - val_loss: 1.5114 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5202 - accuracy: 0.3324 - val_loss: 1.5082 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.0681 - accuracy: 0.3206 - val_loss: 1.5237 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5239 - accuracy: 0.3346 - val_loss: 1.5033 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8219 - accuracy: 0.2333 - val_loss: 1.5096 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5238 - accuracy: 0.3184 - val_loss: 1.5042 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.1209 - accuracy: 0.1599 - val_loss: 1.7673 - val_accuracy: 0.1245\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5545 - accuracy: 0.3346 - val_loss: 1.5055 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8469 - accuracy: 0.2707 - val_loss: 1.5157 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5238 - accuracy: 0.3338 - val_loss: 1.5179 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 2.0793 - accuracy: 0.2509 - val_loss: 1.5509 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5265 - accuracy: 0.3397 - val_loss: 1.5024 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6442 - accuracy: 0.2993 - val_loss: 1.5261 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5270 - accuracy: 0.3177 - val_loss: 1.5187 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7851 - accuracy: 0.2898 - val_loss: 1.5353 - val_accuracy: 0.3154\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5193 - accuracy: 0.3133 - val_loss: 1.5071 - val_accuracy: 0.2863\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.5609 - accuracy: 0.2443 - val_loss: 1.8045 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6541 - accuracy: 0.3272 - val_loss: 1.5176 - val_accuracy: 0.3195\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7476 - accuracy: 0.3030 - val_loss: 1.5942 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5747 - accuracy: 0.3324 - val_loss: 1.5140 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6480 - accuracy: 0.2971 - val_loss: 1.5140 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5176 - accuracy: 0.3375 - val_loss: 1.5064 - val_accuracy: 0.3651\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5577 - accuracy: 0.3199 - val_loss: 1.5000 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5114 - accuracy: 0.3309 - val_loss: 1.4957 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 4.3168 - accuracy: 0.1489 - val_loss: 2.2732 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.7067 - accuracy: 0.3302 - val_loss: 1.5198 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.7154 - accuracy: 0.3067 - val_loss: 1.5669 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5364 - accuracy: 0.3265 - val_loss: 1.4994 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7647 - accuracy: 0.2957 - val_loss: 1.5176 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5290 - accuracy: 0.3338 - val_loss: 1.5268 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6450 - accuracy: 0.3001 - val_loss: 1.5008 - val_accuracy: 0.3320\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5153 - accuracy: 0.3236 - val_loss: 1.5022 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8899 - accuracy: 0.3155 - val_loss: 1.5272 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5238 - accuracy: 0.3324 - val_loss: 1.5037 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6393 - accuracy: 0.2759 - val_loss: 1.5144 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5274 - accuracy: 0.3324 - val_loss: 1.5075 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6109 - accuracy: 0.2847 - val_loss: 1.5125 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5248 - accuracy: 0.3272 - val_loss: 1.5128 - val_accuracy: 0.3112\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6308 - accuracy: 0.2847 - val_loss: 1.5072 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5190 - accuracy: 0.3324 - val_loss: 1.5037 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9583 - accuracy: 0.2333 - val_loss: 1.5193 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5258 - accuracy: 0.3324 - val_loss: 1.5070 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 3.2225 - accuracy: 0.2421 - val_loss: 1.7525 - val_accuracy: 0.3237\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5655 - accuracy: 0.2861 - val_loss: 1.5085 - val_accuracy: 0.3237\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.9011 - accuracy: 0.1878 - val_loss: 1.6047 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5479 - accuracy: 0.3338 - val_loss: 1.5102 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.2056 - accuracy: 0.2803 - val_loss: 1.5217 - val_accuracy: 0.3237\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5365 - accuracy: 0.3111 - val_loss: 1.5007 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 3.2037 - accuracy: 0.2186 - val_loss: 1.9205 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6029 - accuracy: 0.3324 - val_loss: 1.5121 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7021 - accuracy: 0.3243 - val_loss: 1.5096 - val_accuracy: 0.3693\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5201 - accuracy: 0.3316 - val_loss: 1.4961 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.5711 - accuracy: 0.2377 - val_loss: 1.7429 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5807 - accuracy: 0.3324 - val_loss: 1.5142 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.4250 - accuracy: 0.2333 - val_loss: 1.5154 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5159 - accuracy: 0.3037 - val_loss: 1.4986 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6447 - accuracy: 0.2766 - val_loss: 1.5136 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5213 - accuracy: 0.3331 - val_loss: 1.5231 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.7678 - accuracy: 0.2531 - val_loss: 1.5458 - val_accuracy: 0.2158\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5264 - accuracy: 0.3081 - val_loss: 1.5108 - val_accuracy: 0.3195\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.8888 - accuracy: 0.2289 - val_loss: 1.7396 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5686 - accuracy: 0.3390 - val_loss: 1.5029 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7811 - accuracy: 0.2583 - val_loss: 1.5125 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5215 - accuracy: 0.3302 - val_loss: 1.5096 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.5502 - accuracy: 0.2091 - val_loss: 1.7329 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6147 - accuracy: 0.3243 - val_loss: 1.5137 - val_accuracy: 0.3195\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.6342 - accuracy: 0.1577 - val_loss: 1.5376 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5284 - accuracy: 0.3324 - val_loss: 1.5146 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.8026 - accuracy: 0.2318 - val_loss: 1.6329 - val_accuracy: 0.3527\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5842 - accuracy: 0.3280 - val_loss: 1.5188 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5925 - accuracy: 0.3287 - val_loss: 1.5156 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5222 - accuracy: 0.3294 - val_loss: 1.5065 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 4.3669 - accuracy: 0.2751 - val_loss: 2.7214 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8323 - accuracy: 0.3397 - val_loss: 1.5048 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9514 - accuracy: 0.3125 - val_loss: 1.5942 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5637 - accuracy: 0.3338 - val_loss: 1.5143 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9813 - accuracy: 0.2737 - val_loss: 1.5301 - val_accuracy: 0.2863\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 35ms/step - loss: 1.5235 - accuracy: 0.3324 - val_loss: 1.5129 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6024 - accuracy: 0.3118 - val_loss: 1.5206 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5267 - accuracy: 0.3177 - val_loss: 1.5051 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.7408 - accuracy: 0.2472 - val_loss: 1.5250 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5261 - accuracy: 0.3236 - val_loss: 1.5039 - val_accuracy: 0.3320\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9509 - accuracy: 0.3045 - val_loss: 1.5398 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5248 - accuracy: 0.3324 - val_loss: 1.4996 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5553 - accuracy: 0.3294 - val_loss: 1.5038 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.5220 - accuracy: 0.3324 - val_loss: 1.5074 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 2.9249 - accuracy: 0.2142 - val_loss: 1.5478 - val_accuracy: 0.2199\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 35ms/step - loss: 1.5317 - accuracy: 0.3302 - val_loss: 1.5118 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 1.9355 - accuracy: 0.2509 - val_loss: 1.5217 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5173 - accuracy: 0.3324 - val_loss: 1.5047 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5853 - accuracy: 0.3089 - val_loss: 1.5147 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5228 - accuracy: 0.3199 - val_loss: 1.5338 - val_accuracy: 0.3402\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0332 - accuracy: 0.2164 - val_loss: 1.5782 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5441 - accuracy: 0.3324 - val_loss: 1.5034 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.1089 - accuracy: 0.2715 - val_loss: 1.5200 - val_accuracy: 0.3278\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 1.5188 - accuracy: 0.3302 - val_loss: 1.5027 - val_accuracy: 0.3527\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.8941 - accuracy: 0.2957 - val_loss: 1.5113 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5157 - accuracy: 0.3265 - val_loss: 1.5151 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 1.6058 - accuracy: 0.2942 - val_loss: 1.5086 - val_accuracy: 0.3154\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5172 - accuracy: 0.3206 - val_loss: 1.5044 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.9209 - accuracy: 0.3324 - val_loss: 1.9757 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6440 - accuracy: 0.3324 - val_loss: 1.5109 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8568 - accuracy: 0.2766 - val_loss: 1.5108 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5188 - accuracy: 0.3316 - val_loss: 1.5068 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.0182 - accuracy: 0.3206 - val_loss: 1.6972 - val_accuracy: 0.3402\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6170 - accuracy: 0.3316 - val_loss: 1.5271 - val_accuracy: 0.3195\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5726 - accuracy: 0.3221 - val_loss: 1.5154 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5207 - accuracy: 0.3331 - val_loss: 1.5137 - val_accuracy: 0.3278\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5422 - accuracy: 0.3213 - val_loss: 1.5213 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5232 - accuracy: 0.3324 - val_loss: 1.5044 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.9821 - accuracy: 0.2949 - val_loss: 1.5100 - val_accuracy: 0.3485\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5188 - accuracy: 0.3463 - val_loss: 1.5063 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.8232 - accuracy: 0.1944 - val_loss: 1.6254 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5430 - accuracy: 0.2905 - val_loss: 1.5168 - val_accuracy: 0.3485\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6196 - accuracy: 0.3103 - val_loss: 1.5127 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5241 - accuracy: 0.3309 - val_loss: 1.5013 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.7589 - accuracy: 0.2142 - val_loss: 1.5685 - val_accuracy: 0.2241\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5224 - accuracy: 0.3199 - val_loss: 1.5059 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.4815 - accuracy: 0.1255 - val_loss: 1.5821 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5262 - accuracy: 0.3228 - val_loss: 1.5073 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.8009 - accuracy: 0.2619 - val_loss: 1.5206 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5198 - accuracy: 0.3324 - val_loss: 1.5039 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 1.6394 - accuracy: 0.3030 - val_loss: 1.5222 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 1.5228 - accuracy: 0.3324 - val_loss: 1.5321 - val_accuracy: 0.3237\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 2.1340 - accuracy: 0.2803 - val_loss: 1.5340 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5240 - accuracy: 0.3324 - val_loss: 1.5078 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 2.0360 - accuracy: 0.2414 - val_loss: 1.5180 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 1.5233 - accuracy: 0.3302 - val_loss: 1.5034 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.6534 - accuracy: 0.3199 - val_loss: 1.5175 - val_accuracy: 0.3361\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5245 - accuracy: 0.3309 - val_loss: 1.5083 - val_accuracy: 0.3361\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 1.8822 - accuracy: 0.3265 - val_loss: 1.5160 - val_accuracy: 0.3361\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5228 - accuracy: 0.3324 - val_loss: 1.5028 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.8684 - accuracy: 0.2781 - val_loss: 1.5197 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5316 - accuracy: 0.3147 - val_loss: 1.5137 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 1.6082 - accuracy: 0.3118 - val_loss: 1.5159 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5256 - accuracy: 0.3324 - val_loss: 1.5129 - val_accuracy: 0.3444\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 2s 34ms/step - loss: 1.6103 - accuracy: 0.2986 - val_loss: 1.5345 - val_accuracy: 0.2116\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5354 - accuracy: 0.3133 - val_loss: 1.5131 - val_accuracy: 0.3237\n",
      "Epoch 1/2\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.6961 - accuracy: 0.1988 - val_loss: 1.6888 - val_accuracy: 0.3444\n",
      "Epoch 2/2\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 1.5670 - accuracy: 0.3324 - val_loss: 1.5098 - val_accuracy: 0.3444\n",
      "Stopping search: maximum iterations reached --> 30\n"
     ]
    }
   ],
   "source": [
    "lb = [16, 32, 64]\n",
    "ub = [64, 128, 128]\n",
    "\n",
    "xopt, fopt = pso(train_network, lb, ub, swarmsize=5, omega=0.5, phip=0.5, phig=1.0, maxiter=30, minstep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae574c1f-ddcb-455d-a57f-bfd58ec28e39",
   "metadata": {},
   "source": [
    "#### **Results of the PSO Optimized Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77f2ebb1-4dc9-4549-9c5a-c4fe880a0e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The otpimized parameters are:\n",
      "Hidden Channels Space Convolution:  35\n",
      "Hidden Dims Dense Layer1:  110\n",
      "Hidden Dims Dense Layer2:  100\n"
     ]
    }
   ],
   "source": [
    "# The optimized parameters\n",
    "print(\"The otpimized parameters are:\")\n",
    "print(\"Hidden Channels Space Convolution: \", int(xopt[0]))\n",
    "print(\"Hidden Dims Dense Layer1: \", int(xopt[1]))\n",
    "print(\"Hidden Dims Dense Layer2: \", int(xopt[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e94eea-9878-4b8a-a371-329ffc12734c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Running of the Model with Optimized Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26ef80f8-2591-4f6a-bf1b-039e0a372fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_params = [35, 110, 100]\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edeea8f7-8e05-4fab-b412-f196f2b0a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6003c195-20e1-46d1-83ec-8c9961daa2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 2.9121 - accuracy: 0.2487 - val_loss: 1.6479 - val_accuracy: 0.3444\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5408 - accuracy: 0.3324 - val_loss: 1.5012 - val_accuracy: 0.3444\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5151 - accuracy: 0.3324 - val_loss: 1.5046 - val_accuracy: 0.3444\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5115 - accuracy: 0.3368 - val_loss: 1.4933 - val_accuracy: 0.3527\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5093 - accuracy: 0.3419 - val_loss: 1.4947 - val_accuracy: 0.3527\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5069 - accuracy: 0.3441 - val_loss: 1.4924 - val_accuracy: 0.3734\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5070 - accuracy: 0.3595 - val_loss: 1.4871 - val_accuracy: 0.3610\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5063 - accuracy: 0.3566 - val_loss: 1.4846 - val_accuracy: 0.3734\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5037 - accuracy: 0.3639 - val_loss: 1.4865 - val_accuracy: 0.3900\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 1.4980 - accuracy: 0.3712 - val_loss: 1.4809 - val_accuracy: 0.3734\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=10, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed032d12-bcf4-42f0-834d-e3016d63322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_174\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Dense)           multiple                  1089      \n",
      "                                                                 \n",
      " haversine_layer_175 (Havers  multiple                 0         \n",
      " ineLayer)                                                       \n",
      "                                                                 \n",
      " concatenate_174 (Concatenat  multiple                 0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " reshape_174 (Reshape)       multiple                  0         \n",
      "                                                                 \n",
      " multi_head_attention_174 (M  multiple                 9486      \n",
      " ultiHeadAttention)                                              \n",
      "                                                                 \n",
      " flatten_174 (Flatten)       multiple                  0         \n",
      "                                                                 \n",
      " conv1d_174 (Conv1D)         multiple                  140       \n",
      "                                                                 \n",
      " global_max_pooling1d_174 (G  multiple                 0         \n",
      " lobalMaxPooling1D)                                              \n",
      "                                                                 \n",
      " dense_522 (Dense)           multiple                  3960      \n",
      "                                                                 \n",
      " dense_523 (Dense)           multiple                  11100     \n",
      "                                                                 \n",
      " dense_524 (Dense)           multiple                  505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,280\n",
      "Trainable params: 26,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d45f1-c005-401c-a183-7e9d586be798",
   "metadata": {},
   "source": [
    "### **Visualization of the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "474c3db3-bfa3-4521-af3f-7c2d649d4f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXdUlEQVR4nO3dd3xT5f4H8M9J2qRNR7ppC4XSsjeyhF6GikDBXosbq1BFcbQMEa8gG5GKinKFK0MRHFS4cFk/pgUZiiDIEgTZtBUou3sn5/dHmkPTPZKeNPm8X6+8SJ4855xvaLQfnvM85wiiKIogIiIishEKuQsgIiIiMieGGyIiIrIpDDdERERkUxhuiIiIyKYw3BAREZFNYbghIiIim8JwQ0RERDaF4YaIiIhsCsMNERER2RSGG6I6FB0djeDg4BptO2PGDAiCYN6CrMyVK1cgCAJWrFhR58cWBAEzZsyQXq9YsQKCIODKlSuVbhscHIzo6Giz1lOb7wqRvWO4IYLhF1tVHnv27JG7VLs3ZswYCIKACxculNtn8uTJEAQBf/zxRx1WVn3Xrl3DjBkzcPz4cblLkRgD5ieffCJ3KUQ15iB3AUTW4LvvvjN5/e233yIhIaFUe+vWrWt1nC+//BJ6vb5G206ZMgUTJ06s1fFtQVRUFBYsWID4+HhMmzatzD4//PAD2rdvjw4dOtT4OC+++CKee+45qNXqGu+jMteuXcPMmTMRHByMTp06mbxXm+8Kkb1juCEC8MILL5i8PnjwIBISEkq1l5SdnQ2NRlPl4zg6OtaoPgBwcHCAgwP/k+3RoweaNWuGH374ocxwc+DAAVy+fBkffvhhrY6jVCqhVCprtY/aqM13hcje8bQUURX169cP7dq1w5EjR9CnTx9oNBq89957AICNGzdiyJAhCAwMhFqtRmhoKN5//33odDqTfZScR1H8FMDSpUsRGhoKtVqNbt264fDhwybbljXnRhAExMbGYsOGDWjXrh3UajXatm2L7du3l6p/z5496Nq1K5ycnBAaGoolS5ZUeR7Pzz//jKeffhqNGzeGWq1GUFAQ3nrrLeTk5JT6fK6urrh69SoiIyPh6uoKX19fTJgwodTfRWpqKqKjo6HVauHh4YERI0YgNTW10loAw+jNX3/9haNHj5Z6Lz4+HoIgYNiwYcjPz8e0adPQpUsXaLVauLi4oHfv3ti9e3elxyhrzo0oipg9ezYaNWoEjUaDhx56CH/++Wepbe/evYsJEyagffv2cHV1hbu7O8LDw3HixAmpz549e9CtWzcAwEsvvSSd+jTONyprzk1WVhbefvttBAUFQa1Wo2XLlvjkk08giqJJv+p8L2rq5s2bGDlyJBo0aAAnJyd07NgR33zzTal+q1atQpcuXeDm5gZ3d3e0b98e//73v6X3CwoKMHPmTDRv3hxOTk7w9vbGP/7xDyQkJJitVrI//GcgUTXcuXMH4eHheO655/DCCy+gQYMGAAy/CF1dXTF+/Hi4urrip59+wrRp05Ceno6PP/640v3Gx8cjIyMDr732GgRBwEcffYQnnngCly5dqvRf8L/88gvWrVuHN998E25ubvj888/x5JNPIikpCd7e3gCAY8eOYdCgQQgICMDMmTOh0+kwa9Ys+Pr6Vulzr1mzBtnZ2XjjjTfg7e2NQ4cOYcGCBfj777+xZs0ak746nQ4DBw5Ejx498Mknn2Dnzp2YN28eQkND8cYbbwAwhITHH38cv/zyC15//XW0bt0a69evx4gRI6pUT1RUFGbOnIn4+Hg88MADJsf+73//i969e6Nx48a4ffs2vvrqKwwbNgyvvvoqMjIysGzZMgwcOBCHDh0qdSqoMtOmTcPs2bMxePBgDB48GEePHsWAAQOQn59v0u/SpUvYsGEDnn76aTRt2hQ3btzAkiVL0LdvX5w+fRqBgYFo3bo1Zs2ahWnTpmHUqFHo3bs3AKBXr15lHlsURfzzn//E7t27MXLkSHTq1Ak7duzAO++8g6tXr+Kzzz4z6V+V70VN5eTkoF+/frhw4QJiY2PRtGlTrFmzBtHR0UhNTcXYsWMBAAkJCRg2bBgeeeQRzJ07FwBw5swZ7N+/X+ozY8YMxMXF4ZVXXkH37t2Rnp6O33//HUePHsWjjz5aqzrJjolEVEpMTIxY8j+Pvn37igDExYsXl+qfnZ1dqu21114TNRqNmJubK7WNGDFCbNKkifT68uXLIgDR29tbvHv3rtS+ceNGEYD4f//3f1Lb9OnTS9UEQFSpVOKFCxekthMnTogAxAULFkhtERERokajEa9evSq1nT9/XnRwcCi1z7KU9fni4uJEQRDExMREk88HQJw1a5ZJ386dO4tdunSRXm/YsEEEIH700UdSW2Fhodi7d28RgLh8+fJKa+rWrZvYqFEjUafTSW3bt28XAYhLliyR9pmXl2ey3b1798QGDRqIL7/8skk7AHH69OnS6+XLl4sAxMuXL4uiKIo3b94UVSqVOGTIEFGv10v93nvvPRGAOGLECKktNzfXpC5RNPys1Wq1yd/N4cOHy/28Jb8rxr+z2bNnm/R76qmnREEQTL4DVf1elMX4nfz444/L7TN//nwRgPj9999Lbfn5+WLPnj1FV1dXMT09XRRFURw7dqzo7u4uFhYWlruvjh07ikOGDKmwJqLq4mkpompQq9V46aWXSrU7OztLzzMyMnD79m307t0b2dnZ+Ouvvyrd77PPPgtPT0/ptfFf8ZcuXap02/79+yM0NFR63aFDB7i7u0vb6nQ67Ny5E5GRkQgMDJT6NWvWDOHh4ZXuHzD9fFlZWbh9+zZ69eoFURRx7NixUv1ff/11k9e9e/c2+Sxbt26Fg4ODNJIDGOa4jB49ukr1AIZ5Un///Tf27dsntcXHx0OlUuHpp5+W9qlSqQAAer0ed+/eRWFhIbp27VrmKa2K7Ny5E/n5+Rg9erTJqbxx48aV6qtWq6FQGP73qtPpcOfOHbi6uqJly5bVPq7R1q1boVQqMWbMGJP2t99+G6IoYtu2bSbtlX0vamPr1q3w9/fHsGHDpDZHR0eMGTMGmZmZ2Lt3LwDAw8MDWVlZFZ5i8vDwwJ9//onz58/Xui4iI4Ybompo2LCh9MuyuD///BNDhw6FVquFu7s7fH19pcnIaWlple63cePGJq+NQefevXvV3ta4vXHbmzdvIicnB82aNSvVr6y2siQlJSE6OhpeXl7SPJq+ffsCKP35nJycSp3uKl4PACQmJiIgIACurq4m/Vq2bFmlegDgueeeg1KpRHx8PAAgNzcX69evR3h4uElQ/Oabb9ChQwdpPoevry+2bNlSpZ9LcYmJiQCA5s2bm7T7+vqaHA8wBKnPPvsMzZs3h1qtho+PD3x9ffHHH39U+7jFjx8YGAg3NzeTduMKPmN9RpV9L2ojMTERzZs3lwJcebW8+eabaNGiBcLDw9GoUSO8/PLLpeb9zJo1C6mpqWjRogXat2+Pd955x+qX8JP1Y7ghqobiIxhGqamp6Nu3L06cOIFZs2bh//7v/5CQkCDNMajKct7yVuWIJSaKmnvbqtDpdHj00UexZcsWvPvuu9iwYQMSEhKkia8lP19drTDy8/PDo48+iv/9738oKCjA//3f/yEjIwNRUVFSn++//x7R0dEIDQ3FsmXLsH37diQkJODhhx+26DLrOXPmYPz48ejTpw++//577NixAwkJCWjbtm2dLe+29PeiKvz8/HD8+HFs2rRJmi8UHh5uMreqT58+uHjxIr7++mu0a9cOX331FR544AF89dVXdVYn2R5OKCaqpT179uDOnTtYt24d+vTpI7VfvnxZxqru8/Pzg5OTU5kXvavoQnhGJ0+exLlz5/DNN99g+PDhUnttVrM0adIEu3btQmZmpsnozdmzZ6u1n6ioKGzfvh3btm1DfHw83N3dERERIb2/du1ahISEYN26dSankqZPn16jmgHg/PnzCAkJkdpv3bpVajRk7dq1eOihh7Bs2TKT9tTUVPj4+Eivq3PF6SZNmmDnzp3IyMgwGb0xnvY01lcXmjRpgj/++AN6vd5k9KasWlQqFSIiIhAREQG9Xo8333wTS5YswdSpU6WRQy8vL7z00kt46aWXkJmZiT59+mDGjBl45ZVX6uwzkW3hyA1RLRn/hVz8X8T5+fn44osv5CrJhFKpRP/+/bFhwwZcu3ZNar9w4UKpeRrlbQ+Yfj5RFE2W81bX4MGDUVhYiEWLFkltOp0OCxYsqNZ+IiMjodFo8MUXX2Dbtm144okn4OTkVGHtv/32Gw4cOFDtmvv37w9HR0csWLDAZH/z588v1VepVJYaIVmzZg2uXr1q0ubi4gIAVVoCP3jwYOh0OixcuNCk/bPPPoMgCFWeP2UOgwcPRkpKClavXi21FRYWYsGCBXB1dZVOWd65c8dkO4VCIV1YMS8vr8w+rq6uaNasmfQ+UU1w5Iaolnr16gVPT0+MGDFCujXAd999V6fD/5WZMWMGfvzxR4SFheGNN96Qfkm2a9eu0kv/t2rVCqGhoZgwYQKuXr0Kd3d3/O9//6vV3I2IiAiEhYVh4sSJuHLlCtq0aYN169ZVez6Kq6srIiMjpXk3xU9JAcBjjz2GdevWYejQoRgyZAguX76MxYsXo02bNsjMzKzWsYzX64mLi8Njjz2GwYMH49ixY9i2bZvJaIzxuLNmzcJLL72EXr164eTJk1i5cqXJiA8AhIaGwsPDA4sXL4abmxtcXFzQo0cPNG3atNTxIyIi8NBDD2Hy5Mm4cuUKOnbsiB9//BEbN27EuHHjTCYPm8OuXbuQm5tbqj0yMhKjRo3CkiVLEB0djSNHjiA4OBhr167F/v37MX/+fGlk6ZVXXsHdu3fx8MMPo1GjRkhMTMSCBQvQqVMnaX5OmzZt0K9fP3Tp0gVeXl74/fffsXbtWsTGxpr185CdkWeRFpF1K28peNu2bcvsv3//fvHBBx8UnZ2dxcDAQPFf//qXuGPHDhGAuHv3bqlfeUvBy1p2ixJLk8tbCh4TE1Nq2yZNmpgsTRZFUdy1a5fYuXNnUaVSiaGhoeJXX30lvv3226KTk1M5fwv3nT59Wuzfv7/o6uoq+vj4iK+++qq0tLj4MuYRI0aILi4upbYvq/Y7d+6IL774ouju7i5qtVrxxRdfFI8dO1blpeBGW7ZsEQGIAQEBpZZf6/V6cc6cOWKTJk1EtVotdu7cWdy8eXOpn4MoVr4UXBRFUafTiTNnzhQDAgJEZ2dnsV+/fuKpU6dK/X3n5uaKb7/9ttQvLCxMPHDggNi3b1+xb9++JsfduHGj2KZNG2lZvvGzl1VjRkaG+NZbb4mBgYGio6Oj2Lx5c/Hjjz82WZpu/CxV/V6UZPxOlvf47rvvRFEUxRs3bogvvfSS6OPjI6pUKrF9+/alfm5r164VBwwYIPr5+YkqlUps3Lix+Nprr4nXr1+X+syePVvs3r276OHhITo7O4utWrUSP/jgAzE/P7/COokqIoiiFf3zkojqVGRkJJfhEpHN4ZwbIjtR8lYJ58+fx9atW9GvXz95CiIishCO3BDZiYCAAERHRyMkJASJiYlYtGgR8vLycOzYsVLXbiEiqs84oZjITgwaNAg//PADUlJSoFar0bNnT8yZM4fBhohsDkduiIiIyKZwzg0RERHZFIYbIiIisil2N+dGr9fj2rVrcHNzq9alz4mIiEg+oigiIyMDgYGBpW7aWpLdhZtr164hKChI7jKIiIioBpKTk9GoUaMK+9hduDFeFjw5ORnu7u4yV0NERERVkZ6ejqCgIJMbx5bH7sKN8VSUu7s7ww0REVE9U5UpJZxQTERERDaF4YaIiIhsCsMNERER2RS7m3NDRES1p9PpUFBQIHcZZGNUKlWly7yrguGGiIiqTBRFpKSkIDU1Ve5SyAYpFAo0bdoUKpWqVvthuCEioiozBhs/Pz9oNBpeDJXMxniR3evXr6Nx48a1+m4x3BARUZXodDop2Hh7e8tdDtkgX19fXLt2DYWFhXB0dKzxfjihmIiIqsQ4x0aj0chcCdkq4+konU5Xq/0w3BARUbXwVBRZirm+Www3REREZFMYboiIiKopODgY8+fPr3L/PXv2QBAErjKrIww3RERkswRBqPAxY8aMGu338OHDGDVqVJX79+rVC9evX4dWq63R8aqKIcqAq6XMKC27ACnpuWjpX/kdS4mIyPKuX78uPV+9ejWmTZuGs2fPSm2urq7Sc1EUodPp4OBQ+a9GX1/fatWhUqng7+9frW2o5jhyYybnbmSg46wf8fTiXyGKotzlEBERAH9/f+mh1WohCIL0+q+//oKbmxu2bduGLl26QK1W45dffsHFixfx+OOPo0GDBnB1dUW3bt2wc+dOk/2WPC0lCAK++uorDB06FBqNBs2bN8emTZuk90uOqKxYsQIeHh7YsWMHWrduDVdXVwwaNMgkjBUWFmLMmDHw8PCAt7c33n33XYwYMQKRkZE1/vu4d+8ehg8fDk9PT2g0GoSHh+P8+fPS+4mJiYiIiICnpydcXFzQtm1bbN26Vdo2KioKvr6+cHZ2RvPmzbF8+fIa12JJDDdm0thLA0EA0nMLcTszX+5yiIjqhCiKyM4vrPOHOf8ROXHiRHz44Yc4c+YMOnTogMzMTAwePBi7du3CsWPHMGjQIERERCApKanC/cycORPPPPMM/vjjDwwePBhRUVG4e/duuf2zs7PxySef4LvvvsO+ffuQlJSECRMmSO/PnTsXK1euxPLly7F//36kp6djw4YNtfqs0dHR+P3337Fp0yYcOHAAoihi8ODB0jL/mJgY5OXlYd++fTh58iTmzp0rjW5NnToVp0+fxrZt23DmzBksWrQIPj4+tarHUnhaykycHJUI8tQg6W42Lt7KhK+bWu6SiIgsLqdAhzbTdtT5cU/PGgiNyjy/wmbNmoVHH31Ueu3l5YWOHTtKr99//32sX78emzZtQmxsbLn7iY6OxrBhwwAAc+bMweeff45Dhw5h0KBBZfYvKCjA4sWLERoaCgCIjY3FrFmzpPcXLFiASZMmYejQoQCAhQsXSqMoNXH+/Hls2rQJ+/fvR69evQAAK1euRFBQEDZs2ICnn34aSUlJePLJJ9G+fXsAQEhIiLR9UlISOnfujK5duwIwjF5ZK47cmFGorwsA4OKtTJkrISKiqjL+sjbKzMzEhAkT0Lp1a3h4eMDV1RVnzpypdOSmQ4cO0nMXFxe4u7vj5s2b5fbXaDRSsAGAgIAAqX9aWhpu3LiB7t27S+8rlUp06dKlWp+tuDNnzsDBwQE9evSQ2ry9vdGyZUucOXMGADBmzBjMnj0bYWFhmD59Ov744w+p7xtvvIFVq1ahU6dO+Ne//oVff/21xrVYGkduzCjU1xW7z97CpVtZcpdCRFQnnB2VOD1roCzHNRcXFxeT1xMmTEBCQgI++eQTNGvWDM7OznjqqaeQn1/xlIOStwsQBAF6vb5a/eWes/nKK69g4MCB2LJlC3788UfExcVh3rx5GD16NMLDw5GYmIitW7ciISEBjzzyCGJiYvDJJ5/IWnNZOHJjRiG+hvOSHLkhInshCAI0Koc6f1jyKsn79+9HdHQ0hg4divbt28Pf3x9Xrlyx2PHKotVq0aBBAxw+fFhq0+l0OHr0aI332bp1axQWFuK3336T2u7cuYOzZ8+iTZs2UltQUBBef/11rFu3Dm+//Ta+/PJL6T1fX1+MGDEC33//PebPn4+lS5fWuB5L4siNGfG0FBFR/de8eXOsW7cOEREREAQBU6dOrXAExlJGjx6NuLg4NGvWDK1atcKCBQtw7969KgW7kydPws3t/mVJBEFAx44d8fjjj+PVV1/FkiVL4ObmhokTJ6Jhw4Z4/PHHAQDjxo1DeHg4WrRogXv37mH37t1o3bo1AGDatGno0qUL2rZti7y8PGzevFl6z9ow3JhRqJ9h5ObveznILdDByYzDpkREVDc+/fRTvPzyy+jVqxd8fHzw7rvvIj09vc7rePfdd5GSkoLhw4dDqVRi1KhRGDhwIJTKyn+39OnTx+S1UqlEYWEhli9fjrFjx+Kxxx5Dfn4++vTpg61bt0qnyHQ6HWJiYvD333/D3d0dgwYNwmeffQbAcK2eSZMm4cqVK3B2dkbv3r2xatUq839wMxBEuU/w1bH09HRotVqkpaXB3d3drPsWRRGdZiUgLacA28b2RusA8+6fiEhOubm5uHz5Mpo2bQonJye5y7E7er0erVu3xjPPPIP3339f7nIsoqLvWHV+f3POjRkJgsBTU0REZBaJiYn48ssvce7cOZw8eRJvvPEGLl++jOeff17u0qwew42ZhRZNKuaKKSIiqg2FQoEVK1agW7duCAsLw8mTJ7Fz506rnediTTjnxsy4YoqIiMwhKCgI+/fvl7uMeokjN2bG01JERETyYrgxM+OKqYs3s6DX29VcbSIiIqvAcGNmjb00cFAIyCnQISU9V+5yiIiI7A7DjZk5KhVo4q0BwFNTREREcmC4sQDjiqmLNxluiIiI6hrDjQUYV0xdus3l4ERERHWN4cYCuGKKiMi29OvXD+PGjZNeBwcHY/78+RVuIwgCNmzYUOtjm2s/9oThxgKKr5giIiL5REREYNCgQWW+9/PPP0MQBPzxxx/V3u/hw4cxatSo2pZnYsaMGejUqVOp9uvXryM8PNysxyppxYoV8PDwsOgx6hLDjQWE+hjCTUp6LjLzCmWuhojIfo0cORIJCQn4+++/S723fPlydO3aFR06dKj2fn19faHRaMxRYqX8/f2hVqvr5Fi2guHGArQaR/i4Gr6Il3hqiohINo899hh8fX2xYsUKk/bMzEysWbMGI0eOxJ07dzBs2DA0bNgQGo0G7du3xw8//FDhfkueljp//jz69OkDJycntGnTBgkJCaW2effdd9GiRQtoNBqEhIRg6tSpKCgoAGAYOZk5cyZOnDgBQRAgCIJUc8nTUidPnsTDDz8MZ2dneHt7Y9SoUcjMvP+7Jjo6GpGRkfjkk08QEBAAb29vxMTESMeqiaSkJDz++ONwdXWFu7s7nnnmGdy4cUN6/8SJE3jooYfg5uYGd3d3dOnSBb///jsAwz2yIiIi4OnpCRcXF7Rt2xZbt26tcS1VwdsvWEiorwtuZ+bh4q1MdGjkIXc5RESWIYpAQXbdH9dRAwhCpd0cHBwwfPhwrFixApMnT4ZQtM2aNWug0+kwbNgwZGZmokuXLnj33Xfh7u6OLVu24MUXX0RoaCi6d+9e6TH0ej2eeOIJNGjQAL/99hvS0tJM5ucYubm5YcWKFQgMDMTJkyfx6quvws3NDf/617/w7LPP4tSpU9i+fTt27twJANBqtaX2kZWVhYEDB6Jnz544fPgwbt68iVdeeQWxsbEmAW737t0ICAjA7t27ceHCBTz77LPo1KkTXn311Uo/T1mfzxhs9u7di8LCQsTExODZZ5/Fnj17AABRUVHo3LkzFi1aBKVSiePHj8PR0REAEBMTg/z8fOzbtw8uLi44ffo0XF1dq11HdcgabuLi4rBu3Tr89ddfcHZ2Rq9evTB37ly0bNmywu3mz5+PRYsWISkpCT4+PnjqqacQFxdX6vbocgr1c8Vvl+/yBppEZNsKsoE5gXV/3PeuASqXKnV9+eWX8fHHH2Pv3r3o168fAMMpqSeffBJarRZarRYTJkyQ+o8ePRo7duzAf//73yqFm507d+Kvv/7Cjh07EBho+LuYM2dOqXkyU6ZMkZ4HBwdjwoQJWLVqFf71r3/B2dkZrq6ucHBwgL+/f7nHio+PR25uLr799lu4uBg+/8KFCxEREYG5c+eiQYMGAABPT08sXLgQSqUSrVq1wpAhQ7Br164ahZtdu3bh5MmTuHz5MoKCggAA3377Ldq2bYvDhw+jW7duSEpKwjvvvINWrVoBAJo3by5tn5SUhCeffBLt27cHAISEhFS7huqS9bTU3r17ERMTg4MHDyIhIQEFBQUYMGAAsrLKDwTx8fGYOHEipk+fjjNnzmDZsmVYvXo13nvvvTqsvHIhPlwxRURkDVq1aoVevXrh66+/BgBcuHABP//8M0aOHAkA0Ol0eP/999G+fXt4eXnB1dUVO3bsQFJSUpX2f+bMGQQFBUnBBgB69uxZqt/q1asRFhYGf39/uLq6YsqUKVU+RvFjdezYUQo2ABAWFga9Xo+zZ89KbW3btoVSqZReBwQE4ObNm9U6VvFjBgUFScEGANq0aQMPDw+cOXMGADB+/Hi88sor6N+/Pz788ENcvHhR6jtmzBjMnj0bYWFhmD59eo0mcFeXrCM327dvN3m9YsUK+Pn54ciRI+jTp0+Z2/z6668ICwvD888/D8CQfocNG4bffvvN4vVWB1dMEZFdcNQYRlHkOG41jBw5EqNHj8Z//vMfLF++HKGhoejbty8A4OOPP8a///1vzJ8/H+3bt4eLiwvGjRuH/Px8s5V74MABREVFYebMmRg4cCC0Wi1WrVqFefPmme0YxRlPCRkJggC9Xm+RYwGGlV7PP/88tmzZgm3btmH69OlYtWoVhg4dildeeQUDBw7Eli1b8OOPPyIuLg7z5s3D6NGjLVaPVU0oTktLAwB4eXmV26dXr144cuQIDh06BAC4dOkStm7disGDB5fZPy8vD+np6SaPutCs6EJ+l29nQccbaBKRrRIEw+mhun5UYb5Ncc888wwUCgXi4+Px7bff4uWXX5bm3+zfvx+PP/44XnjhBXTs2BEhISE4d+5clffdunVrJCcn4/r161LbwYMHTfr8+uuvaNKkCSZPnoyuXbuiefPmSExMNOmjUqmg0+kqPdaJEydMznDs378fCoWi0ikdNWX8fMnJyVLb6dOnkZqaijZt2khtLVq0wFtvvYUff/wRTzzxBJYvXy69FxQUhNdffx3r1q3D22+/jS+//NIitRpZTbjR6/UYN24cwsLC0K5du3L7Pf/885g1axb+8Y9/wNHREaGhoejXr1+5p6Xi4uKkc6pardZkWM2SAj2coXZQIF+nx9/3ZJhsR0REEldXVzz77LOYNGkSrl+/jujoaOm95s2bIyEhAb/++ivOnDmD1157zWQlUGX69++PFi1aYMSIEThx4gR+/vlnTJ482aRP8+bNkZSUhFWrVuHixYv4/PPPsX79epM+wcHBuHz5Mo4fP47bt28jLy+v1LGioqLg5OSEESNG4NSpU9i9ezdGjx6NF198UZpvU1M6nQ7Hjx83eZw5cwb9+/dH+/btERUVhaNHj+LQoUMYPnw4+vbti65duyInJwexsbHYs2cPEhMTsX//fhw+fBitW7cGAIwbNw47duzA5cuXcfToUezevVt6z1KsJtzExMTg1KlTWLVqVYX99uzZgzlz5uCLL77A0aNHsW7dOmzZsgXvv/9+mf0nTZqEtLQ06VE8eVqSUiGgKefdEBFZjZEjR+LevXsYOHCgyfyYKVOm4IEHHsDAgQPRr18/+Pv7IzIyssr7VSgUWL9+PXJyctC9e3e88sor+OCDD0z6/POf/8Rbb72F2NhYdOrUCb/++iumTp1q0ufJJ5/EoEGD8NBDD8HX17fM5egajQY7duzA3bt30a1bNzz11FN45JFHsHDhwur9ZZQhMzMTnTt3NnlERERAEARs3LgRnp6e6NOnD/r374+QkBCsXr0aAKBUKnHnzh0MHz4cLVq0wDPPPIPw8HDMnDkTgCE0xcTEoHXr1hg0aBBatGiBL774otb1VkQQRVH2cyaxsbHYuHEj9u3bh6ZNm1bYt3fv3njwwQfx8ccfS23ff/+9tM5foag4r6Wnp0Or1SItLQ3u7u5mqb88MfFHseWP65gypDVe6W352eFERJaUm5uLy5cvo2nTpla1OpVsR0Xfser8/pZ1QrEoihg9ejTWr1+PPXv2VBpsACA7O7tUgDHOCLeCnGYilCM3REREdU7WcBMTE4P4+Hhs3LgRbm5uSElJAWC4cJGzszMAYPjw4WjYsCHi4uIAGO4T8umnn6Jz587o0aMHLly4gKlTpyIiIsJk2Zs14IopIiKiuidruFm0aBEASBdVMlq+fLk02SspKclkpGbKlCkQBAFTpkzB1atX4evri4iIiFLnN61BaNGKKY7cEBER1R3ZT0tVxnhpZyMHBwdMnz4d06dPt1BV5mOcUHwnKx/3svLh6aKSuSIiIiLbZzWrpWyRi9oBgVrDhKhLtzl6Q0S2wdrmN5LtMNd3i+HGwjjvhohshfGqt9nZvHYXWYbxqtC1nUPLu4JbWIiPC34+fxsXOXJDRPWcUqmEh4eHdI8ijUYjXeWXqLb0ej1u3boFjUYDB4faxROGGwvjyA0R2RLjHatrehNGooooFAo0bty41qGZ4cbCjCumLnHFFBHZAEEQEBAQAD8/PxQUFMhdDtkYlUpV6cV4q4LhxsKM4SbxbjbyC/VQOXCaExHVf0ql0uquLUZkxN+0FtbAXQ0XlRI6vYikuzw1RUREZGkMNxYmCII07+YC590QERFZHMNNHQgpupgfr3VDRERkeQw3dUC6DQNHboiIiCyO4aYOSMvBuWKKiIjI4hhu6kDxG2jysuVERESWxXBTB5p4a6AQgIzcQtzKzJO7HCIiIpvGcFMHnByVCPLSAOC8GyIiIktjuKkj0pWKuWKKiIjIohhu6ohxOThHboiIiCyL4aaOcMUUERFR3WC4qSPFV0wRERGR5TDc1JFQX8NpqaupOcjJ18lcDRERke1iuKkjXi4qeGgcIYrA5ducd0NERGQpDDd1RBAEnpoiIiKqAww3dUi6geYtjtwQERFZCsNNHeKKKSIiIstjuKlDPC1FRERkeQw3dci4YurSrSzo9byBJhERkSUw3NShIC8NHJUCcgp0uJ6eK3c5RERENonhpg45KhVo4m28DQNPTREREVkCw00du79iiuGGiIjIEhhu6tj9FVNcDk5ERGQJDDd1jCumiIiILIvhpo4ZV0wx3BAREVkGw00dCykaubmRnoeM3AKZqyEiIrI9DDd1TOvsCF83NQDehoGIiMgSGG5kIF3M7zZPTREREZkbw40MjKemLt7kyA0REZG5MdzIgCumiIiILEfWcBMXF4du3brBzc0Nfn5+iIyMxNmzZyvdLjU1FTExMQgICIBarUaLFi2wdevWOqjYPLhiioiIyHIc5Dz43r17ERMTg27duqGwsBDvvfceBgwYgNOnT8PFxaXMbfLz8/Hoo4/Cz88Pa9euRcOGDZGYmAgPD4+6Lb4WjCM3V25no1Cnh4OSA2hERETmImu42b59u8nrFStWwM/PD0eOHEGfPn3K3Obrr7/G3bt38euvv8LR0REAEBwcbOlSzaqhhzPUDgrkFerx970cBPuUHeSIiIio+qxqyCAtLQ0A4OXlVW6fTZs2oWfPnoiJiUGDBg3Qrl07zJkzBzqdrsz+eXl5SE9PN3nITaEQpEnFXDFFRERkXlYTbvR6PcaNG4ewsDC0a9eu3H6XLl3C2rVrodPpsHXrVkydOhXz5s3D7Nmzy+wfFxcHrVYrPYKCgiz1EaolxDjvhiumiIiIzMpqwk1MTAxOnTqFVatWVdhPr9fDz88PS5cuRZcuXfDss89i8uTJWLx4cZn9J02ahLS0NOmRnJxsifKrjSumiIiILEPWOTdGsbGx2Lx5M/bt24dGjRpV2DcgIACOjo5QKpVSW+vWrZGSkoL8/HyoVCqT/mq1Gmq12iJ11wZXTBEREVmGrCM3oigiNjYW69evx08//YSmTZtWuk1YWBguXLgAvV4vtZ07dw4BAQGlgo01uz9yw9NSRERE5iRruImJicH333+P+Ph4uLm5ISUlBSkpKcjJyZH6DB8+HJMmTZJev/HGG7h79y7Gjh2Lc+fOYcuWLZgzZw5iYmLk+Ag1ZpxzczcrH3ez8mWuhoiIyHbIGm4WLVqEtLQ09OvXDwEBAdJj9erVUp+kpCRcv35deh0UFIQdO3bg8OHD6NChA8aMGYOxY8di4sSJcnyEGtOoHNDQwxkAcImnpoiIiMxG1jk3oihW2mfPnj2l2nr27ImDBw9aoKK6FeLrgqupObh0Kwtdg8tf/k5ERERVZzWrpewRV0wRERGZH8ONjLhiioiIyPwYbmTEFVNERETmx3Ajo1A/Q7hJupuNvMKybx9BRERE1cNwIyM/NzVc1Q7Q6UUk3cmWuxwiIiKbwHAjI0EQis274akpIiIic2C4kVkIV0wRERGZFcONzLhiioiIyLwYbmTGFVNERETmxXAjM+OKqUs3M6t0xWYiIiKqGMONzJp4a6AQgIy8QtzKyJO7HCIionqP4UZmagclGntpAPDUFBERkTkw3FgBrpgiIiIyH4YbK8AVU0RERObDcGMFuGKKiIjIfBhurIBxxdTFmxy5ISIiqi2GGytgHLm5mpqDnHzeQJOIiKg2GG6sgJeLCp4aRwDApdscvSEiIqoNhhsrYVwxdYnzboiIiGqF4cZKcMUUERGReTDcWAmumCIiIjIPhhsrIYUbrpgiIiKqFYYbKyHdQPN2JvR63kCTiIiophhurESQpzMclQJyC/S4lpYjdzlERET1FsONlXBQKtDE2zCpmCumiIiIao7hxopwxRQREVHtMdxYkVDeHZyIiKjWGG6syP0VUzwtRUREVFMMN1ZEuoEmR26IiIhqjOHGioQUzbm5mZGHjNwCmashIiKqnxhurIi7kyP83NQAuGKKiIiophhurEwIV0wRERHVCsONleGKKSIiotphuLEyXDFFRERUOww3VoYrpoiIiGqH4cbKGK9SfOVOFgp1epmrISIiqn9kDTdxcXHo1q0b3Nzc4Ofnh8jISJw9e7bK269atQqCICAyMtJyRdaxQK0znBwVKNCJ+Pseb6BJRERUXbKGm7179yImJgYHDx5EQkICCgoKMGDAAGRlVT7f5MqVK5gwYQJ69+5dB5XWHYVCQFMfnpoiIiKqKQc5D759+3aT1ytWrICfnx+OHDmCPn36lLudTqdDVFQUZs6ciZ9//hmpqakWrrRuhfq64Mz1dFy8lYlHWjeQuxwiIqJ6xarm3KSlpQEAvLy8Kuw3a9Ys+Pn5YeTIkZXuMy8vD+np6SYPa8cVU0RERDVnNeFGr9dj3LhxCAsLQ7t27crt98svv2DZsmX48ssvq7TfuLg4aLVa6REUFGSuki2GK6aIiIhqzmrCTUxMDE6dOoVVq1aV2ycjIwMvvvgivvzyS/j4+FRpv5MmTUJaWpr0SE5ONlfJFhPKqxQTERHVmKxzboxiY2OxefNm7Nu3D40aNSq338WLF3HlyhVERERIbXq9Ybm0g4MDzp49i9DQUJNt1Go11Gq1ZQq3kJCiCcX3sgtwNysfXi4qmSsiIiKqP2QNN6IoYvTo0Vi/fj327NmDpk2bVti/VatWOHnypEnblClTkJGRgX//+9/14pRTVTirlGjo4YyrqTm4dCsTXi4Vz0EiIiKi+2QNNzExMYiPj8fGjRvh5uaGlJQUAIBWq4WzszMAYPjw4WjYsCHi4uLg5ORUaj6Oh4cHAFQ4T6c+CvF1wdXUHFy8lYmuwQw3REREVSXrnJtFixYhLS0N/fr1Q0BAgPRYvXq11CcpKQnXr1+XsUp53L+BJldMERERVYfsp6Uqs2fPngrfX7FihXmKsTLSiqmbnFRMRERUHVazWopMccUUERFRzTDcWKlmRaelku5mI69QJ3M1RERE9QfDjZXydVPDTe0AvQgk3cmWuxwiIqJ6g+HGSgmCgBCemiIiIqo2hhsrxhVTRERE1cdwY8W4YoqIiKj6GG6sGFdMERERVR/DjRUrflqqKtcEIiIiIoYbq9bYWwOlQkBmXiFuZeTJXQ4REVG9wHBjxdQOSgR5Gu6xdYGnpoiIiKqE4cbKccUUERFR9TDcWDmumCIiIqoehhsrxxVTRERE1cNwY+WMp6Uu8bQUERFRlTDcWDljuLmamoPs/EKZqyEiIrJ+DDdWztNFBU+NIwDg8m2O3hAREVWG4aYe4IopIiKiqmO4qQekcMMVU0RERJViuKkHQv24YoqIiKiqGG7qAZ6WIiIiqjqGm3rAGG4u386EXs8baBIREVWE4aYeaOTpDJVSgdwCPa6l5chdDhERkVVjuKkHHJQKNPHWAOCpKSIiosow3NQTXDFFRERUNQw39QRXTBEREVUNw009cX/FFMMNERFRRRhu6gkuByciIqoahpt6IsTXcFrqVkYe0nMLZK6GiIjIejHc1BNuTo7wc1MDAC5x9IaIiKhcNQo3ycnJ+Pvvv6XXhw4dwrhx47B06VKzFUalccUUERFR5WoUbp5//nns3r0bAJCSkoJHH30Uhw4dwuTJkzFr1iyzFkj3ccUUERFR5WoUbk6dOoXu3bsDAP773/+iXbt2+PXXX7Fy5UqsWLHCnPVRMVwxRUREVLkahZuCggKo1Yb5Hzt37sQ///lPAECrVq1w/fp181VHJrhiioiIqHI1Cjdt27bF4sWL8fPPPyMhIQGDBg0CAFy7dg3e3t5mLZDuC/UzhJvEO1ko1OllroaIiMg61SjczJ07F0uWLEG/fv0wbNgwdOzYEQCwadMm6XQVmV+AuxOcHBUo0IlIvscbaBIREZXFoSYb9evXD7dv30Z6ejo8PT2l9lGjRkGj0ZitODKlUAgI8XHF6evpuHgzE019XOQuiYiIyOrUaOQmJycHeXl5UrBJTEzE/PnzcfbsWfj5+VV5P3FxcejWrRvc3Nzg5+eHyMhInD17tsJtvvzyS/Tu3Ruenp7w9PRE//79cejQoZp8jHrJeGqKk4qJiIjKVqNw8/jjj+Pbb78FAKSmpqJHjx6YN28eIiMjsWjRoirvZ+/evYiJicHBgweRkJCAgoICDBgwAFlZ5U+Y3bNnD4YNG4bdu3fjwIEDCAoKwoABA3D16tWafJR6J9SXy8GJiIgqUqNwc/ToUfTu3RsAsHbtWjRo0ACJiYn49ttv8fnnn1d5P9u3b0d0dDTatm2Ljh07YsWKFUhKSsKRI0fK3WblypV488030alTJ7Rq1QpfffUV9Ho9du3aVZOPUu9wxRQREVHFajTnJjs7G25ubgCAH3/8EU888QQUCgUefPBBJCYm1riYtLQ0AICXl1e1aikoKCh3m7y8POTl5Umv09PTa1yfNeC1boiIiCpWo5GbZs2aYcOGDUhOTsaOHTswYMAAAMDNmzfh7u5eo0L0ej3GjRuHsLAwtGvXrsrbvfvuuwgMDET//v3LfD8uLg5arVZ6BAUF1ag+a2GcRJyaXYC7WfkyV0NERGR9ahRupk2bhgkTJiA4OBjdu3dHz549ARhGcTp37lyjQmJiYnDq1CmsWrWqytt8+OGHWLVqFdavXw8nJ6cy+0yaNAlpaWnSIzk5uUb1WQtnlRINPZwBcPSGiIioLDU6LfXUU0/hH//4B65fvy5d4wYAHnnkEQwdOrTa+4uNjcXmzZuxb98+NGrUqErbfPLJJ/jwww+xc+dOdOjQodx+arVaupqyrQj1c8XV1BxcvJmJbsFVP4VHRERkD2oUbgDA398f/v7+0t3BGzVqVO0L+ImiiNGjR2P9+vXYs2cPmjZtWqXtPvroI3zwwQfYsWMHunbtWu3a67tQXxfsO3eLIzdERERlqNFpKb1ej1mzZkGr1aJJkyZo0qQJPDw88P7770Ovr/ptAWJiYvD9998jPj4ebm5uSElJQUpKCnJy7l99d/jw4Zg0aZL0eu7cuZg6dSq+/vprBAcHS9tkZtrPL3qumCIiIipfjUZuJk+ejGXLluHDDz9EWFgYAOCXX37BjBkzkJubiw8++KBK+zFeE6dfv34m7cuXL0d0dDQAICkpCQqFwmSb/Px8PPXUUybbTJ8+HTNmzKjJx6l3uGKKiIiofIIoimJ1NwoMDMTixYulu4Ebbdy4EW+++aZVX1AvPT0dWq0WaWlpNV7ZJbebGbno/sEuKATgzPuDoHZQyl0SERGRRVXn93eNTkvdvXsXrVq1KtXeqlUr3L17tya7pGrwdVXDTe0AvQgk3smWuxwiIiKrUqNw07FjRyxcuLBU+8KFCytcuUTmIQgCQoz3mLrJU1NERETF1WjOzUcffYQhQ4Zg586d0jVuDhw4gOTkZGzdutWsBVLZQn1dcCI5lfNuiIiISqjRyE3fvn1x7tw5DB06FKmpqUhNTcUTTzyBP//8E9999525a6QycMUUERFR2Wp8nZvAwMBSq6JOnDiBZcuWYenSpbUujCrGFVNERERlq9HIDcmvmZ/hHlOXbmWhBgveiIiIbBbDTT3V2MsFSoWAzLxC3MzIq3wDIiIiO8FwU0+pHBRo7KUBwBVTRERExVVrzs0TTzxR4fupqam1qYWqKdTXBZdvZ+HirUz0auYjdzlERERWoVrhRqvVVvr+8OHDa1UQVV2oryt2nrnJFVNERETFVCvcLF++3FJ1UA1wxRQREVFpnHNTj4UWrZjinBsiIqL7GG7qsRAfw8jNtbRcZOcXylwNERGRdWC4qcc8XVTwclEBMFzvhoiIiBhu6r1Q36JTU5x3Q0REBIDhpt7jPaaIiIhMMdzUc1wxRUREZIrhpp7jiikiIiJTDDf1nHHk5vLtLOj1vIEmERERw00918hTA5VSgbxCPa6m5shdDhERkewYbuo5pUJAsE/RDTQ574aIiIjhxhZwxRQREdF9DDc2gCumiIiI7mO4sQFcMUVERHQfw40NMI7cXLrN01JEREQMNzagqY9h5OZWRh7ScgpkroaIiEheDDc2wM3JEQ3c1QCAS5x3Q0REdo7hxkZwxRQREZEBw42N4IopIiIiA4YbGxHqyxVTREREAMONzQj148gNERERwHBjM0KKTksl3c1GgU4vczVERETyYbixEQHuTnB2VKJAJyL5brbc5RAREcmG4cZGKBQCQozzbrhiioiI7BjDjQ3hiikiIiKGG5sihRuumCIiIjsma7iJi4tDt27d4ObmBj8/P0RGRuLs2bOVbrdmzRq0atUKTk5OaN++PbZu3VoH1Vo/6QaaHLkhIiI7Jmu42bt3L2JiYnDw4EEkJCSgoKAAAwYMQFZW+XNGfv31VwwbNgwjR47EsWPHEBkZicjISJw6daoOK7dOIT73r1IsiqLM1RAREclDEK3ot+CtW7fg5+eHvXv3ok+fPmX2efbZZ5GVlYXNmzdLbQ8++CA6deqExYsXV3qM9PR0aLVapKWlwd3d3Wy1W4OcfB3aTN8OUQSOTOkPb1e13CURERGZRXV+f1vVnJu0tDQAgJeXV7l9Dhw4gP79+5u0DRw4EAcOHCizf15eHtLT000etspZpURDD2cAXDFFRET2y2rCjV6vx7hx4xAWFoZ27dqV2y8lJQUNGjQwaWvQoAFSUlLK7B8XFwetVis9goKCzFq3teGKKSIisndWE25iYmJw6tQprFq1yqz7nTRpEtLS0qRHcnKyWfdvbbhiioiI7J2D3AUAQGxsLDZv3ox9+/ahUaNGFfb19/fHjRs3TNpu3LgBf3//Mvur1Wqo1fYz94QrpoiIyN7JOnIjiiJiY2Oxfv16/PTTT2jatGml2/Ts2RO7du0yaUtISEDPnj0tVWa9Yhy5uXSbc26IiMg+yTpyExMTg/j4eGzcuBFubm7SvBmtVgtnZ8PE2OHDh6Nhw4aIi4sDAIwdOxZ9+/bFvHnzMGTIEKxatQq///47li5dKtvnsCbGWzAk381GboEOTo5KmSsiIiKqW7KO3CxatAhpaWno168fAgICpMfq1aulPklJSbh+/br0ulevXoiPj8fSpUvRsWNHrF27Fhs2bKhwErI98XVVw83JAXoRSLzDG2gSEZH9kXXkpiqX2NmzZ0+ptqeffhpPP/20BSqq/wRBQKivK44np+LirUy09HeTuyQiIqI6ZTWrpch8uGKKiIjsGcONDeKKKSIismcMNzaIK6aIiMieMdzYoNCiFVMXb2byBppERGR3GG5sUGMvFygVArLydbiRnid3OURERHWK4cYGqRwUaOKlAcB5N0REZH8YbmxUCG+gSUREdorhxkZJK6a4HJyIiOwMw42Nkq51c4srpoiIyL4w3Ngo44qpSzwtRUREdobhxkaF+BhGbq6l5SIrr1DmaoiIiOoOw42N8nRRwdtFBQC4zIv5ERGRHWG4sWGhXDFFRER2iOHGhnHFFBER2SOGGxvGFVNERGSPGG5sGE9LERGRPWK4sWEhRcvBL9/Ogk7PG2gSEZF9YLixYY08NVApFcgr1ONaao7c5RAREdUJhhsbplQIaOpjGL25wFNTRERkJxhubBxXTBERkb1huLFxXDFFRET2huHGxhnDDe8xRURE9oLhxsYZV0xx5IaIiOwFw42NCykaubmdmYe07AKZqyEiIrI8hhsb56p2gL+7EwDg4m2emiIiItvHcGMHuGKKiIjsCcONHeCKKSIisicMN3aA95giIiJ7wnBjB4wrprgcnIiI7AHDjR0wjtwk3slGgU4vczVERESWxXBjB/zdnaBRKVGoF5F0N1vucoiIiCyK4cYOKBTC/Yv5ccUUERHZOIYbO8EVU0REZC8YbuwEV0wREZG9YLixE7yBJhER2QuGGztR/AaaoijKXA0REZHlyBpu9u3bh4iICAQGBkIQBGzYsKHSbVauXImOHTtCo9EgICAAL7/8Mu7cuWP5Yuu5pj4uEAQgLacAd7Ly5S6HiIjIYmQNN1lZWejYsSP+85//VKn//v37MXz4cIwcORJ//vkn1qxZg0OHDuHVV1+1cKX1n5OjEo08nQFwxRQREdk2BzkPHh4ejvDw8Cr3P3DgAIKDgzFmzBgAQNOmTfHaa69h7ty5lirRpoT6uiL5bg4u3spCjxBvucshIiKyiHo156Znz55ITk7G1q1bIYoibty4gbVr12Lw4MHlbpOXl4f09HSTh73iiikiIrIH9SrchIWFYeXKlXj22WehUqng7+8PrVZb4WmtuLg4aLVa6REUFFSHFVsXrpgiIiJ7UK/CzenTpzF27FhMmzYNR44cwfbt23HlyhW8/vrr5W4zadIkpKWlSY/k5OQ6rNi6FF8xRUREZKtknXNTXXFxcQgLC8M777wDAOjQoQNcXFzQu3dvzJ49GwEBAaW2UavVUKvVdV2qVTKO3CTfy0ZugQ5OjkqZKyIiIjK/ejVyk52dDYXCtGSl0vALmtduqZyPqwruTg4QReDKHY7eEBGRbZI13GRmZuL48eM4fvw4AODy5cs4fvw4kpKSABhOKQ0fPlzqHxERgXXr1mHRokW4dOkS9u/fjzFjxqB79+4IDAyU4yPUK4IgINSvaFLxTYYbIiKyTbKelvr999/x0EMPSa/Hjx8PABgxYgRWrFiB69evS0EHAKKjo5GRkYGFCxfi7bffhoeHBx5++GEuBa+GUF9XHEtK5YopIiKyWYJoZ+dz0tPTodVqkZaWBnd3d7nLqXOL9lzE3O1/4fFOgfj3c53lLoeIiKhKqvP7u17NuaHaM66YusQVU0REZKMYbuxM8Qv52dmgHRER2QmGGzvTxFsDB4WA7HwdUtJz5S6HiIjI7Bhu7IyjUoHG3hoAXDFFRES2ieHGDvEeU0REZMsYbuwQww0REdkyhhs7xBVTRERkyxhu7BBHboiIyJYx3Nih0KKRm+tpucjMK5S5GiIiIvNiuLFDHhoVfFxVAIDLPDVFREQ2huHGToXw1BQREdkohhs7xXk3RERkqxhu7FQoV0wREZGNYrixUxy5ISIiW8VwY6eM4ebS7Szo9LyBJhER2Q6GGzvV0NMZKgcF8gv1uHovR+5yiIiIzIbhxk4pFQJCfAzzbnhqioiIbAnDjR3jvBsiIrJFDDd2zLhi6iJXTBERkQ1huLFjvJAfERHZIoYbOyatmGK4ISIiG8JwY8dCik5L3c7MR2p2vszVEBERmQfDjR1zUTsgQOsEgPNuiIjIdjDc2DmumCIiIlvDcGPn7q+YYrghIiLbwHBj50KkScU8LUVERLaB4cbO8bQUERHZGoYbczqyArh2TO4qqiXUz3BaKulONgp0epmrISIiqj2GG3O5dgzY8jbwVX9gz1xAVyB3RVXi7+4EjUqJQr2IxDvZcpdDRERUaww35uLRBGj1GKAvBPbMAZYNAG6dk7uqSgmCwFNTRERkUxhuzEXjBTy9AnhyGeCkBa4dBZb0Bg4uBvTWfbqHK6aIiMiWMNyYkyAA7Z8C3jwIhD4MFOYC298FvnscSE2Wu7pyhXLFFBER2RCGG0twDwReWAcMmQc4aoDL+4BFvYDjPwCiKHd1pfAGmkREZEsYbixFEIBurwCv/wI06g7kpQMbXgdWvwBk3Za7OhPGFVMXb2ZCtMLwRUREVB0MN5bmHQq8tA14ZBqgcAT+2gx88SDw1xa5K5MEe7tAEID03ELczuQNNImIqH5juKkLSgeg99vAqz8Bfm2ArFvAqueBDW8CuWlyVwcnRyWCPDUAeGqKiIjqP1nDzb59+xAREYHAwEAIgoANGzZUuk1eXh4mT56MJk2aQK1WIzg4GF9//bXlizWHgA7AqD1A2FgAAnB8JbAozDAnR2ZcMUVERLZC1nCTlZWFjh074j//+U+Vt3nmmWewa9cuLFu2DGfPnsUPP/yAli1bWrBKM3NQA4/OMpyq8gwG0pKBbyKA7ZOAghzZyuKKKSIishUOch48PDwc4eHhVe6/fft27N27F5cuXYKXlxcAIDg42ELVWViTnsDr+4EfpwBHlgMHvwAu7ASGLgEaPlDn5XDFFBER2Yp6Nedm06ZN6Nq1Kz766CM0bNgQLVq0wIQJE5CTU/6IR15eHtLT000eVkPtCkTMB55fA7j6A7fPFd2+4cM6v30DT0sREZGtqFfh5tKlS/jll19w6tQprF+/HvPnz8fatWvx5ptvlrtNXFwctFqt9AgKCqrDiquoxQDgzQNA26GAqAP2xAHLHq3T2zeE+hlGbv6+l4PcAl2dHZeIiMjc6lW40ev1EAQBK1euRPfu3TF48GB8+umn+Oabb8odvZk0aRLS0tKkR3KylV4p2OT2DR6GG3Eu6Q0cXFQnt2/wdlFB6+wIUQQu3+a8GyIiqr/qVbgJCAhAw4YNodVqpbbWrVtDFEX8/fffZW6jVqvh7u5u8rBq7Z8yjOKEPlJ0+4aJwLf/tPjtGww30OSpKSIiqv/qVbgJCwvDtWvXkJl5/5fvuXPnoFAo0KhRIxkrMzP3QOCF/wFDPjXcvuHKz0W3b4i36O0bpLuD3+TIDRER1V+yhpvMzEwcP34cx48fBwBcvnwZx48fR1JSEgDDKaXhw4dL/Z9//nl4e3vjpZdewunTp7Fv3z688847ePnll+Hs7CzHR7AcQQC6jSxx+4Y3DLdvyLxlkUMaV0xdus2RGyIiqr9kDTe///47OnfujM6dOwMAxo8fj86dO2PatGkAgOvXr0tBBwBcXV2RkJCA1NRUdO3aFVFRUYiIiMDnn38uS/11wjsUeHk78Mh0i9++gaeliIjIFgiind0pMT09HVqtFmlpadY//6aklJPAuteAm38aXneKAgbFAU7aireroou3MvHIvL1wdlTiz5kDoVAIZtkvERFRbVXn93e9mnNj9/zbA6N2A2HjYInbNzT20sBBISCnQIeU9Fyz7JOIiKiuMdzUNw5q4NGZFrl9g6NSgSbevIEmERHVbww39ZXx9g1dXjK8PvgFsKQPcPVorXZ7f8UUww0REdVPDDf1mfH2DVFrTW/fsDuuxrdvMF6p+BIv5EdERPUUw40taP5o0e0bnjDcvmHvh4aQc+tstXcV4sMVU0REVL8x3NgKjRfw9PL7t2+4ftxwmqqat28wjtycv5HJe0wREVG9xKXgtij9OrApFriw0/A6uDcQ+QXg0bjSTdOyC9Bx1o/Sa183NRp6OBsens4mzwM9nKF1drTUpyAiIpJU5/c3w42tEkXg96+BH6cABdmAyg0Inwt0et5w9eMKvLv2D2w6cQ05VRi5cVM73A89RYHH+LyRhzN8XNW8Xg4REdUaw00F7CbcGN25aLhtQ/JvhtcthwAR/wZcfSvcTBRF3MsuwLXUHPx9LwdXU3Nw9V4OrqZm41pqLq6m5uBuVn6lh1cpFQj0cDIJPcX/DNA6Q+XAs6NERFQxhpsK2F24AQC9Dvj1c+CnDwB9AaDxMQSc1o/VarfZ+YVS+DEEnuyiAGQIQinpudBX8u0SBMDPeOrLU4NADyc0ksKPBg09neGqdqhVnUREVP8x3FTALsONUcpJYP3rwI1ThtcdnwfCPzTb7RtKKtDpcSM91yTwXE0t9riXg7zCyic7a50dpZGfRmWcAvNxVUGo5FQbERHVbww3FbDrcAMAhXnAnjhg/78BUQ+4NzJMNg7pW+eliKKIO1n5UugpfQosB2k5lV+vR+2guB94tKanvnxc1XBUClAqBDgqFVAqBDgoBDgoFYY/FYb3GI6IiKwbw00F7D7cGCUdNIzi3LtseN3jDaD/dMDRWd66SsjMM5z6unovB38XCz3GthsZuTDHN1gKPQrTIGQaiAQoFQrpuaG/oqi96LlCgFIpwFFh6OsovXc/UCmLhyul8Zgl+hbt10Fx//gKBaAQDH0UgvEBKZwpFQKUggChqM3QF4b3BGOIQ5nbK4z9hZJ9wOBHRFaB4aYCDDfF5GUCCVMNq6oAwKcFMHQx0LCLvHVVQ36hHilpufjbONG5aNKzcfTnblY+dHoRhXpR+pOqRyEYQpUhAAFKoVgYKmozhiVl8RBWKigV618UxIx/GgOW0iSYCdJxlcW2ud9XMAlximL7u/8+yuhb+rOU3i9M+iqNYVABqW95KvqGVfR/28r+V1zj/Va4pYEAw+cRBECAIdAaM60A4wLLkm1CsfcM+5Cel2gz7LfkMYx7KnFcoFibYLK4UxDKP67x56U0/pyK/byNP2eq3xhuKsBwU4bzCcDGWCAzBRCUQJ93gD4TAGUZ17DR6wF9YYmHrpLXZT0q2UZXWf+qHEcHCApA5Qqo3QC1K0SVG/QqV+gcXaFTuULn4IpCBxcUOrqgwMEVBUoNCpTO0IkCCnR66PQiCnTGYKRHoe5+SCrU6Q1/Fmsv0IvQSe1FbUX7Kb7N/f0W9dWV0bfEfkXR8FwvAnq9CL0oQieKRc8BnbGPKEKnh/S8+Pt60fgwvCayJ0rF/bBjDNrGtuKBWupXPCwrTMNw8VBsuo+yg5XxT4cSfUuGb5WDAiqlwvBnyecOCqiLXjuW6KN2MH3toLS9VagMNxVguClH9l1g6wTg1P8Mr9XugEJZOkCIVb/acf0lGMJQsVB0/7V7idduZbQVe+2oqfS6QnIqHpLEosCjE0WIehiCUVE40oklQpUxZEnPRUPurTBw3e+nK7ZfnbRPmLQZw5xONG2X6tHfD3ImdRZvL9VWWQ3339OV/Kwl9lXeT7W803gVfgvKebO6x6h4m7LbRfH+qJAoFo3zlNFm6CdKo0T3/xSL9b3fR0TxEaXibWJRS1H/Yse83y6abF98nxDvj0YV3yezuimFgGLhSHk//BQLRY5KASoHpWk4KhGmjK/L3t40cBXfxslRiUAP805zqM7vb66xJQONF/DU10DLwcCWt4Hc1OptLygBhUOxh9Iw8mN8XvI9k9fltVX0fonXynL663VAfiaQl2E4DZeXAeRnFL0uo03UAxCBvHTDI6OWf6+CwnABRWPYMQlFJV8bg1EZfRycDHWJoqFGUSx6rS/Wpi+nDfefG9uL+ilEPRSiCAeprar7LKMvxHKOU6JmQWF4KJRFz5UlnitKtBe9p1AUe64s8by8/VVyHLIZxoBTPJRKYVR6DpO2wuJ9TcKsIRyXta9CfeXBvPQ+AZ1eb/izZBAvse9CnYh8nR75hXrkFeqLnuuQLz0v9tAV9Sl6XnyoQi8CuQV65BboARTW+c/Dx1WF36c8WufHNWK4IVPtnwJaDAJSE0sEBceKg4YVj05UmSgCBTlFYSezKOCUFYoyK+iTeT84GX/Z56UZHmR9ygxL5YWoEoFLqTIEeKUKcFAVvS7xkNodAaX6/nMHdYl2436KPS+3vfjD0Tb+2zMDw8R5VDgfypYZRzqLB6C8koGoovcKdcjX6VGgE+8HpkI98nW6UtvnlbG/ks/dnOS9NQ/DDZWmdgUatJW7ironCIBKY3igQe32JYpAflaxsJNedgAq67VJW6YhVFVceNFohHB/tKJUm1BJP2O7UEZbOf3KPE4lxwaKje7oDSNroq7Ec9E87VX6ORX11Vd+yQGrVe1QVSI8GcOa9GexEKdwKN1WZl+HEu8patm3rGNV0Lfkd0uvK/Y9M34v9KXbTPqK5bSX/L6WtY+K9l2F/RZvVzgU/fzUxX5e6nLa1FL4FZQqOChVcHBQQ6NSyfudtAIMN0SWIAhFp5lcATf/2u1Lrwd0eSgzdPBf7eUrHnqkXyg60184pdpL/gIqp11faAhEhfmArsSjrDaT9gLDz9P4vDCvRHtBUf9iz6V95AElVz8Z3yMyEpQmwed+6K1hW5X6FwtgSkfDZUVq+/++WmC4IbJ2CgWgsK7rD9ULgmCYi2VL/5szBjZdfuVBqEpBK79EACwW5oyLCUq2ldm3sOztTfoaw2JhGW1V6Gs2Qom5WMVOQwpCOe3G05Il25TF/qFR1jwvRTltxbYr2VdfWCzwGn9Weff/LK/N5HuiM9wwuSDbjH9v1eTiC7xzQbbD29B/9URENs4Y2JQOADRyV1O3KgtCEKoYNmxwtFMU74/8ScEnr5K2vBLBuCpt+abPywxhRW2O8n4/GW6IiMj6KRQAFGVff8veCYLhlJCDClDLXYx14FpIIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENoXhhoiIiGwKww0RERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENsVB7gLqmiiKAID09HSZKyEiIqKqMv7eNv4er4jdhZuMjAwAQFBQkMyVEBERUXVlZGRAq9VW2EcQqxKBbIher8e1a9fg5uYGQRDMuu/09HQEBQUhOTkZ7u7uZt03VR9/HtaFPw/rw5+JdeHPo2KiKCIjIwOBgYFQKCqeVWN3IzcKhQKNGjWy6DHc3d35xbQi/HlYF/48rA9/JtaFP4/yVTZiY8QJxURERGRTGG6IiIjIpjDcmJFarcb06dOhVqvlLoXAn4e14c/D+vBnYl348zAfu5tQTERERLaNIzdERERkUxhuiIiIyKYw3BAREZFNYbghIiIim8JwYyb/+c9/EBwcDCcnJ/To0QOHDh2SuyS7FRcXh27dusHNzQ1+fn6IjIzE2bNn5S6Linz44YcQBAHjxo2TuxS7dfXqVbzwwgvw9vaGs7Mz2rdvj99//13usuySTqfD1KlT0bRpUzg7OyM0NBTvv/9+le6fROVjuDGD1atXY/z48Zg+fTqOHj2Kjh07YuDAgbh586bcpdmlvXv3IiYmBgcPHkRCQgIKCgowYMAAZGVlyV2a3Tt8+DCWLFmCDh06yF2K3bp37x7CwsLg6OiIbdu24fTp05g3bx48PT3lLs0uzZ07F4sWLcLChQtx5swZzJ07Fx999BEWLFggd2n1GpeCm0GPHj3QrVs3LFy4EIDh/lVBQUEYPXo0Jk6cKHN1dOvWLfj5+WHv3r3o06eP3OXYrczMTDzwwAP44osvMHv2bHTq1Anz58+Xuyy7M3HiROzfvx8///yz3KUQgMceewwNGjTAsmXLpLYnn3wSzs7O+P7772WsrH7jyE0t5efn48iRI+jfv7/UplAo0L9/fxw4cEDGysgoLS0NAODl5SVzJfYtJiYGQ4YMMflvherepk2b0LVrVzz99NPw8/ND586d8eWXX8pdlt3q1asXdu3ahXPnzgEATpw4gV9++QXh4eEyV1a/2d2NM83t9u3b0Ol0aNCggUl7gwYN8Ndff8lUFRnp9XqMGzcOYWFhaNeundzl2K1Vq1bh6NGjOHz4sNyl2L1Lly5h0aJFGD9+PN577z0cPnwYY8aMgUqlwogRI+Quz+5MnDgR6enpaNWqFZRKJXQ6HT744ANERUXJXVq9xnBDNi0mJganTp3CL7/8Incpdis5ORljx45FQkICnJyc5C7H7un1enTt2hVz5swBAHTu3BmnTp3C4sWLGW5k8N///hcrV65EfHw82rZti+PHj2PcuHEIDAzkz6MWGG5qycfHB0qlEjdu3DBpv3HjBvz9/WWqigAgNjYWmzdvxr59+9CoUSO5y7FbR44cwc2bN/HAAw9IbTqdDvv27cPChQuRl5cHpVIpY4X2JSAgAG3atDFpa926Nf73v//JVJF9e+eddzBx4kQ899xzAID27dsjMTERcXFxDDe1wDk3taRSqdClSxfs2rVLatPr9di1axd69uwpY2X2SxRFxMbGYv369fjpp5/QtGlTuUuya4888ghOnjyJ48ePS4+uXbsiKioKx48fZ7CpY2FhYaUujXDu3Dk0adJEporsW3Z2NhQK01/FSqUSer1epopsA0duzGD8+PEYMWIEunbtiu7du2P+/PnIysrCSy+9JHdpdikmJgbx8fHYuHEj3NzckJKSAgDQarVwdnaWuTr74+bmVmq+k4uLC7y9vTkPSgZvvfUWevXqhTlz5uCZZ57BoUOHsHTpUixdulTu0uxSREQEPvjgAzRu3Bht27bFsWPH8Omnn+Lll1+Wu7R6jUvBzWThwoX4+OOPkZKSgk6dOuHzzz9Hjx495C7LLgmCUGb78uXLER0dXbfFUJn69evHpeAy2rx5MyZNmoTz58+jadOmGD9+PF599VW5y7JLGRkZmDp1KtavX4+bN28iMDAQw4YNw7Rp06BSqeQur95iuCEiIiKbwjk3REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIhgu/rhhwwa5yyAiM2C4ISLZRUdHQxCEUo9BgwbJXRoR1UO8txQRWYVBgwZh+fLlJm1qtVqmaoioPuPIDRFZBbVaDX9/f5OHp6cnAMMpo0WLFiE8PBzOzs4ICQnB2rVrTbY/efIkHn74YTg7O8Pb2xujRo1CZmamSZ+vv/4abdu2hVqtRkBAAGJjY03ev337NoYOHQqNRoPmzZtj06ZNlv3QRGQRDDdEVC9MnToVTz75JE6cOIGoqCg899xzOHPmDAAgKysLAwcOhKenJw4fPow1a9Zg586dJuFl0aJFiImJwahRo3Dy5Els2rQJzZo1MznGzJkz8cwzz+CPP/7A4MGDERUVhbt379bp5yQiMxCJiGQ2YsQIUalUii4uLiaPDz74QBRFUQQgvv766ybb9OjRQ3zjjTdEURTFpUuXip6enmJmZqb0/pYtW0SFQiGmpKSIoiiKgYGB4uTJk8utAYA4ZcoU6XVmZqYIQNy2bZvZPicR1Q3OuSEiq/DQQw9h0aJFJm1eXl7S8549e5q817NnTxw/fhwAcObMGXTs2BEuLi7S+2FhYdDr9Th79iwEQcC1a9fwyCOPVFhDhw4dpOcuLi5wd3fHzZs3a/qRiEgmDDdEZBVcXFxKnSYyF2dn5yr1c3R0NHktCAL0er0lSiIiC+KcGyKqFw4ePFjqdevWrQEArVu3xokTJ5CVlSW9v3//figUCrRs2RJubm4IDg7Grl276rRmIpIHR26IyCrk5eUhJSXFpM3BwQE+Pj4AgDVr1qBr1674xz/+gZUrV+LQoUNYtmwZACAqKgrTp0/HiBEjMGPGDNy6dQujR4/Giy++iAYNGgAAZsyYgddffx1+fn4IDw9HRkYG9u/fj9GjR9ftByUii2O4ISKrsH37dgQEBJi0tWzZEn/99RcAw0qmVatW4c0330RAQAB++OEHtGnTBgCg0WiwY8cOjB07Ft26dYNGo8GTTz6JTz/9VNrXiBEjkJubi88++wwTJkyAj48Pnnrqqbr7gERUZwRRFEW5iyAiqoggCFi/fj0iIyPlLoWI6gHOuSEiIiKbwnBDRERENoVzbojI6vHsORFVB0duiIiIyKYw3BAREZFNYbghIiIim8JwQ0RERDaF4YaIiIhsCsMNERER2RSGGyIiIrIpDDdERERkUxhuiIiIyKb8PzqiUBDN6OyiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f4f1314-c94b-4b22-b018-bd3e86556880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict classes for test set\n",
    "y_pred = model.predict([x_test, coordinates_test])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2991d417-82a4-4991-8257-c32bc3e987e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d05eb2ba-0779-4199-bc4f-52493600a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0fe8064-fbff-44d1-bd07-531fb2fa85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa724872-1333-45e6-8f92-09105893694f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803867d-85f0-4571-a2c0-f17677e38b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Ablation Studies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a39404-3175-4227-b9de-cb70df285fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Haversine Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d183d52-d36f-47f4-8b87-6afb02c8c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelASHaversine(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModelASHaversine, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=self.num_embedding_dims + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(64, 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = input_coordinates\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([input_features[:, :embedding_output.shape[1]-1], haversine_output])\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(multi_head_attention)\n",
    "        \n",
    "        concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        concat_features = tf.expand_dims(concat_features, axis=-1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        space_conv = self.space_conv(concat_features)\n",
    "        \n",
    "        # Global max pooling\n",
    "        pooled = self.global_pooling_layer(space_conv)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(pooled)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa33facc-a36f-43da-8f20-fd92b0e956d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28751837-3cec-4aef-b68f-b947cb5097c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the multiclass labels\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a83911d-1433-4434-a0ea-0b8c4fa31be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModelASHaversine(num_classes=y_train_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ee9a146-2af7-4d62-b4c3-17cf012e53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db69fdef-ae6e-47e2-84c8-1f21b928d0ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 1/43 [..............................] - ETA: 2s - loss: 6.3869 - accuracy: 0.1562WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 3/43 [=>............................] - ETA: 1s - loss: 6.1820 - accuracy: 0.1146WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 5/43 [==>...........................] - ETA: 1s - loss: 5.4561 - accuracy: 0.1750WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 7/43 [===>..........................] - ETA: 1s - loss: 5.7778 - accuracy: 0.2098WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 9/43 [=====>........................] - ETA: 1s - loss: 5.9243 - accuracy: 0.2153WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 5.6933 - accuracy: 0.2386WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 5.4865 - accuracy: 0.2620WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "15/43 [=========>....................] - ETA: 0s - loss: 5.2965 - accuracy: 0.2667WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 5.0862 - accuracy: 0.2757WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "19/43 [============>.................] - ETA: 0s - loss: 4.9321 - accuracy: 0.2730WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "21/43 [=============>................] - ETA: 0s - loss: 4.6652 - accuracy: 0.2560WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 4.4656 - accuracy: 0.2500WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "25/43 [================>.............] - ETA: 0s - loss: 4.3316 - accuracy: 0.2438WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "27/43 [=================>............] - ETA: 0s - loss: 4.1895 - accuracy: 0.2373WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 4.0500 - accuracy: 0.2338WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 3.9102 - accuracy: 0.2278WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 3.7755 - accuracy: 0.2339WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.6632 - accuracy: 0.2393WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.5525 - accuracy: 0.2424WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 3.4550 - accuracy: 0.2452WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 3.3697 - accuracy: 0.2447WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_as_haversine/embedding/kernel:0', 'my_model_as_haversine/embedding/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 3.3058 - accuracy: 0.2428 - val_loss: 1.6830 - val_accuracy: 0.2075\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2000, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709da455-aab5-41b9-9123-d9181a2d972c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3a94a1e-fc25-43f4-9240-4589ba6a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelASAttention(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModelASAttention, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        # self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=len(df.columns) + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(64, 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = self.haversine_layer(input_coordinates)\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([embedding_output, haversine_output])\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        # multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(reshaped_features_for_attention)\n",
    "        \n",
    "        concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        concat_features = tf.expand_dims(concat_features, axis=-1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        space_conv = self.space_conv(concat_features)\n",
    "        \n",
    "        # Global max pooling\n",
    "        pooled = self.global_pooling_layer(space_conv)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(pooled)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        # print(\"Output Shape: \", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36584029-f876-42f6-bdc5-eb3b7802814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModelASAttention(num_classes=y_train_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "240573c4-6ec9-4fdf-8336-169d99aefb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb84a7ce-d6ad-475b-a399-690d969b3e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 23ms/step - loss: 2.8642 - accuracy: 0.2091 - val_loss: 1.6570 - val_accuracy: 0.2075\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2000, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c22ba6-7352-4956-855d-2c672fcbfe47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Dimensional Expansion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b3e0eb1-a719-4aa7-a196-1591adf94108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelASDimExp(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModelASDimExp, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=self.num_embedding_dims + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(64, 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = self.haversine_layer(input_coordinates)\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([embedding_output, haversine_output])\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(multi_head_attention)\n",
    "        \n",
    "        # concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        concat_features = tf.expand_dims(flattened_attention, axis=-1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        space_conv = self.space_conv(concat_features)\n",
    "        \n",
    "        # Global max pooling\n",
    "        pooled = self.global_pooling_layer(space_conv)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(pooled)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        # print(\"Output Shape: \", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57f9bc8c-d6b3-4b7f-98e2-b7a14e613c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModelASDimExp(num_classes=y_train_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0687d04f-3a89-4741-be27-5ddd324fdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2dc60b1c-6522-49ef-b690-982c709394a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 32ms/step - loss: 1.5978 - accuracy: 0.2597 - val_loss: 1.5176 - val_accuracy: 0.3444\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2000, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5bdb4-4ee1-4dd1-97c6-417f75526692",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Spatial Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "319679b4-8744-4a4e-b4cb-ec257ef2aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelASSpatialConv(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModelASSpatialConv, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=self.num_embedding_dims + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(64, 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = self.haversine_layer(input_coordinates)\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([embedding_output, haversine_output])\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(multi_head_attention)\n",
    "        \n",
    "        concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        # concat_features = tf.expand_dims(concat_features, axis=-1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        # space_conv = self.space_conv(concat_features)\n",
    "        \n",
    "        # Global max pooling\n",
    "        # pooled = self.global_pooling_layer(space_conv)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(concat_features)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        # print(\"Output Shape: \", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "629db5a3-1203-4f91-bccc-daa5052c3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModelASSpatialConv(num_classes=y_train_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c10074d4-b9d9-412b-804c-5c480fe2f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45522b88-453c-47b2-9d08-a3d39e07a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 27ms/step - loss: 7.4441 - accuracy: 0.1431 - val_loss: 3.4829 - val_accuracy: 0.0996\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2000, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a2ca3-485d-4278-b143-b9f5cb77d3b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Spatial Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49b2689d-7c82-47d5-b717-db7d86565571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelASSpatialPool(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModelASSpatialPool, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_embedding_dims = len(df.columns) + 1\n",
    "        \n",
    "        # Define input layers\n",
    "        self.input_features = Input(shape=(len(df.columns),), name='input_features')\n",
    "        self.input_coordinates = Input(shape=(2,), name='input_coordinates')\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        self.embedding = layers.Dense(self.num_embedding_dims, activation='relu', name='embedding')\n",
    "        \n",
    "        # Define custom Haversine layer\n",
    "        self.haversine_layer = HaversineLayer()\n",
    "        \n",
    "        # Define other layers\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.reshape_layer = layers.Reshape((1, self.num_embedding_dims + 1))\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=2, key_dim=self.num_embedding_dims + 1)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.space_conv = layers.Conv1D(64, 3, activation='relu')\n",
    "        self.global_pooling_layer = layers.GlobalMaxPooling1D()\n",
    "        self.dense_layer1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Separate inputs\n",
    "        input_features, input_coordinates = inputs\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding_output = self.embedding(input_features)\n",
    "        \n",
    "        # Haversine layer\n",
    "        haversine_output = self.haversine_layer(input_coordinates)\n",
    "        \n",
    "        # Concatenate features with haversine output\n",
    "        combined_features = self.concat_layer([embedding_output, haversine_output])\n",
    "        \n",
    "        # Reshape features for attention\n",
    "        reshaped_features_for_attention = self.reshape_layer(combined_features)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        multi_head_attention = self.multi_head_attention(reshaped_features_for_attention, reshaped_features_for_attention)\n",
    "        \n",
    "        # Flatten and concatenate attention output with features\n",
    "        flattened_attention = self.flatten_layer(multi_head_attention)\n",
    "        \n",
    "        concat_features = self.concat_layer([flattened_attention, combined_features])\n",
    "        \n",
    "        # Expand dimensions for convolution\n",
    "        concat_features = tf.expand_dims(concat_features, axis=-1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        space_conv = self.space_conv(concat_features)\n",
    "        \n",
    "        pooled = tf.reshape(space_conv, (space_conv.shape[0], space_conv.shape[1] * space_conv.shape[2]))\n",
    "        \n",
    "        # Global max pooling\n",
    "        # pooled = self.global_pooling_layer(space_conv)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        dense1 = self.dense_layer1(pooled)\n",
    "        dense2 = self.dense_layer2(dense1)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(dense2)\n",
    "        # print(\"Output Shape: \", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e48a6f9a-43bf-4a7a-958d-ea779c6dce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModelASSpatialPool(num_classes=y_train_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42a135e8-3032-4a95-b398-48d5ee2d06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b72aa6c2-8ad0-4d71-a102-6a7ec8d77001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 33ms/step - loss: 1.5281 - accuracy: 0.3485 - val_loss: 1.4901 - val_accuracy: 0.3734\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([x_train, coordinates_train], y_train_one_hot, epochs=2000, batch_size=32, validation_split=0.15, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5760fca-5bf7-4ae4-b287-763566cae2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
